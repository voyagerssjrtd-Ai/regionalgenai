{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7671af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV normalized trades: 25, errors: 5\n",
      "JSON normalized trades: 5, errors: 2\n",
      "\n",
      "Sample normalized CSV trade:\n",
      "{'account_id': 'ACC_UNKNOWN',\n",
      " 'counterparty_lei': '5493001KJTIIGC8Y1R12',\n",
      " 'currency': 'USD',\n",
      " 'execution_time': '2024-01-25T10:32:09Z',\n",
      " 'instrument': {'asset_class': 'EQUITY',\n",
      "                'isin': 'US1234567890',\n",
      "                'symbol': 'STK001'},\n",
      " 'notional': 63525.0,\n",
      " 'order_type': 'LIMIT',\n",
      " 'price': 42.35,\n",
      " 'quantity': 1500,\n",
      " 'short_sell_flag': 'N',\n",
      " 'side': 'BUY',\n",
      " 'trade_id': 'INT_A1001',\n",
      " 'trader_id': 'TRDR_UNKNOWN',\n",
      " 'trading_capacity': 'DEAL',\n",
      " 'venue_mic': 'XNAS'}\n",
      "\n",
      "Sample normalized JSON trade:\n",
      "{'account_id': 'ACC_UNKNOWN',\n",
      " 'counterparty_lei': 'LEI55544433322211100099',\n",
      " 'currency': 'USD',\n",
      " 'execution_time': '2024-01-25T10:32:09Z',\n",
      " 'instrument': {'asset_class': 'EQUITY',\n",
      "                'isin': 'US5554443332',\n",
      "                'symbol': 'STK201'},\n",
      " 'notional': 110295.0,\n",
      " 'order_type': 'LIMIT',\n",
      " 'price': 122.55,\n",
      " 'quantity': 900,\n",
      " 'short_sell_flag': 'N',\n",
      " 'side': 'BUY',\n",
      " 'trade_id': 'INT_TX9001',\n",
      " 'trader_id': 'TRDR_UNKNOWN',\n",
      " 'trading_capacity': 'DEAL',\n",
      " 'venue_mic': 'XNAS'}\n",
      "\n",
      "Sample CSV errors:\n",
      "[{'errors': ['Invalid quantity: 0'],\n",
      "  'raw': {'CP_LEI': 'LEI1234567',\n",
      "          'ISIN': 'US9933442211',\n",
      "          'Qty': '0',\n",
      "          'Side': 'SELL',\n",
      "          'Symbol': 'STK012',\n",
      "          'TradeDate': '2024/01/25',\n",
      "          'TradeID': 'A1005',\n",
      "          'TradePrice': '75.10',\n",
      "          'TradeTime': '10:45:22',\n",
      "          'Venue': 'ARCX'}},\n",
      " {'errors': ['Could not parse execution_time'],\n",
      "  'raw': {'CP_LEI': 'LEI44444444444444444444',\n",
      "          'ISIN': 'INVALIDISIN',\n",
      "          'Qty': '700',\n",
      "          'Side': 'B',\n",
      "          'Symbol': 'STK022',\n",
      "          'TradeDate': '01-25-2024',\n",
      "          'TradeID': 'A1008',\n",
      "          'TradePrice': '215.90',\n",
      "          'TradeTime': '11:15:33',\n",
      "          'Venue': 'BATS'}}]\n",
      "\n",
      "Sample JSON errors:\n",
      "[{'errors': ['Missing venue'],\n",
      "  'raw': {'cp': {'lei': ''},\n",
      "          'id': 'TX9003',\n",
      "          'instrument': {'isin': 'INVALID999999', 'symbol': 'STK203'},\n",
      "          'px': 88.1,\n",
      "          'qty': 500,\n",
      "          'side': 'B',\n",
      "          'timestamp': '2024/01/25 10:40:22',\n",
      "          'venue': ''}},\n",
      " {'errors': ['Invalid side: P', 'Invalid price: None'],\n",
      "  'raw': {'cp': {'lei': 'INVALIDLEICODE'},\n",
      "          'id': 'TX9007',\n",
      "          'instrument': {'isin': 'US1122993377', 'symbol': 'STK207'},\n",
      "          'px': '',\n",
      "          'qty': 450,\n",
      "          'side': 'P',\n",
      "          'timestamp': '2024-01-25 11:10:22',\n",
      "          'venue': 'XNSE'}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_11744\\2497643953.py:30: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt = datetime.utcfromtimestamp(ts)\n"
     ]
    }
   ],
   "source": [
    "# normalization_agent.py\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def parse_datetime(value_date: str, value_time: Optional[str] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try multiple datetime formats and return ISO 8601 with Z suffix, or None on failure.\n",
    "    \"\"\"\n",
    "    if value_time:\n",
    "        raw = f\"{value_date} {value_time}\"\n",
    "    else:\n",
    "        raw = str(value_date)\n",
    "\n",
    "    candidates = [\n",
    "        \"%Y/%m/%d %H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%d-%m-%Y %H:%M:%S\",\n",
    "        \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y/%m/%dT%H:%M:%SZ\",\n",
    "    ]\n",
    "\n",
    "    # Unix timestamp case\n",
    "    try:\n",
    "        if isinstance(value_date, int) or (isinstance(value_date, str) and value_date.isdigit()):\n",
    "            ts = int(value_date)\n",
    "            dt = datetime.utcfromtimestamp(ts)\n",
    "            return dt.isoformat() + \"Z\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for fmt in candidates:\n",
    "        try:\n",
    "            dt = datetime.strptime(raw, fmt)\n",
    "            return dt.isoformat() + \"Z\"\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # last resort: try datetime.fromisoformat\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(raw.replace(\"Z\", \"\"))\n",
    "        return dt.isoformat() + \"Z\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_side(raw_side: str) -> Optional[str]:\n",
    "    if not raw_side:\n",
    "        return None\n",
    "    s = raw_side.strip().upper()\n",
    "    if s in (\"B\", \"BUY\"):\n",
    "        return \"BUY\"\n",
    "    if s in (\"S\", \"SELL\"):\n",
    "        return \"SELL\"\n",
    "    return None  # invalid / unexpected\n",
    "\n",
    "\n",
    "def to_float(val) -> Optional[float]:\n",
    "    try:\n",
    "        if val is None or val == \"\":\n",
    "            return None\n",
    "        return float(val)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def to_int(val) -> Optional[int]:\n",
    "    try:\n",
    "        if val is None or val == \"\":\n",
    "            return None\n",
    "        return int(float(val))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_notional(qty: Optional[int], price: Optional[float]) -> Optional[float]:\n",
    "    if qty is None or price is None:\n",
    "        return None\n",
    "    return round(qty * price, 2)\n",
    "\n",
    "\n",
    "# ---------- Normalizers for each RAW format ----------\n",
    "\n",
    "def normalize_row_oms_csv(row: Dict[str, str]) -> Tuple[Optional[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Normalize a single row from raw_large_feed_A.csv into canonical trade.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    trade_id = row.get(\"TradeID\") or row.get(\"id\")\n",
    "    symbol = row.get(\"Symbol\") or row.get(\"ticker\") or row.get(\"SYM\")\n",
    "    isin = row.get(\"ISIN\") or row.get(\"ISIN_Code\")\n",
    "    side = normalize_side(row.get(\"Side\") or row.get(\"SIDE\"))\n",
    "    qty = to_int(row.get(\"Qty\") or row.get(\"quantity\"))\n",
    "    price = to_float(row.get(\"TradePrice\") or row.get(\"px\") or row.get(\"price\"))\n",
    "    venue = row.get(\"Venue\") or row.get(\"Market\") or row.get(\"VENUE\")\n",
    "    cp_lei = (row.get(\"CP_LEI\") or row.get(\"counterparty_lei\") or \"\").strip() or None\n",
    "\n",
    "    # date/time variations\n",
    "    trade_date = row.get(\"TradeDate\") or row.get(\"date\") or row.get(\"dt\")\n",
    "    trade_time = row.get(\"TradeTime\") or row.get(\"time\") or row.get(\"tm\")\n",
    "    execution_time = parse_datetime(trade_date, trade_time) if trade_date else None\n",
    "\n",
    "    if not trade_id:\n",
    "        errors.append(\"Missing trade_id\")\n",
    "    if not execution_time:\n",
    "        errors.append(\"Could not parse execution_time\")\n",
    "    if not isin:\n",
    "        errors.append(\"Missing ISIN\")\n",
    "    if not side:\n",
    "        errors.append(f\"Invalid side: {row.get('Side')}\")\n",
    "    if qty is None or qty <= 0:\n",
    "        errors.append(f\"Invalid quantity: {row.get('Qty')}\")\n",
    "    if price is None or price <= 0:\n",
    "        errors.append(f\"Invalid price: {row.get('TradePrice')}\")\n",
    "    if not venue:\n",
    "        errors.append(\"Missing venue\")\n",
    "\n",
    "    if errors:\n",
    "        return None, errors\n",
    "\n",
    "    notional = compute_notional(qty, price)\n",
    "\n",
    "    canonical = {\n",
    "        \"trade_id\": f\"INT_{trade_id}\",\n",
    "        \"execution_time\": execution_time,\n",
    "        \"instrument\": {\n",
    "            \"isin\": isin,\n",
    "            \"symbol\": symbol,\n",
    "            \"asset_class\": \"EQUITY\"  # you can refine with lookup later\n",
    "        },\n",
    "        \"side\": side,\n",
    "        \"quantity\": qty,\n",
    "        \"price\": price,\n",
    "        \"notional\": notional,\n",
    "        \"venue_mic\": venue,\n",
    "        \"trader_id\": \"TRDR_UNKNOWN\",\n",
    "        \"account_id\": \"ACC_UNKNOWN\",\n",
    "        \"counterparty_lei\": cp_lei,\n",
    "        \"currency\": \"USD\",\n",
    "        \"order_type\": \"LIMIT\" if price is not None else \"MARKET\",\n",
    "        \"trading_capacity\": \"DEAL\",\n",
    "        \"short_sell_flag\": \"N\"\n",
    "    }\n",
    "\n",
    "    return canonical, []\n",
    "\n",
    "\n",
    "def normalize_record_json_batch(rec: Dict[str, Any]) -> Tuple[Optional[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Normalize a record from raw_large_feed_B.json (inside trade_feed[]).\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    trade_id = rec.get(\"id\")\n",
    "    # timestamp could be in 'timestamp' or 'ts'\n",
    "    ts_val = rec.get(\"timestamp\") or rec.get(\"ts\")\n",
    "    execution_time = None\n",
    "\n",
    "    if isinstance(ts_val, (int, float)) or (isinstance(ts_val, str) and ts_val.isdigit()):\n",
    "        execution_time = parse_datetime(int(ts_val))\n",
    "    else:\n",
    "        execution_time = parse_datetime(str(ts_val)) if ts_val else None\n",
    "\n",
    "    instrument = rec.get(\"instrument\") or rec.get(\"inst\") or {}\n",
    "    isin = instrument.get(\"isin\") or instrument.get(\"id\") or instrument.get(\"instrumentId\")\n",
    "    symbol = instrument.get(\"symbol\") or instrument.get(\"ticker\")\n",
    "\n",
    "    # side may be in root or inside order\n",
    "    side_raw = rec.get(\"side\")\n",
    "    if not side_raw and rec.get(\"order\"):\n",
    "        side_raw = rec[\"order\"].get(\"side\")\n",
    "\n",
    "    side = normalize_side(side_raw) if side_raw else None\n",
    "\n",
    "    qty = rec.get(\"qty\")\n",
    "    if qty is None and rec.get(\"order\"):\n",
    "        qty = rec[\"order\"].get(\"shares\")\n",
    "    qty = to_int(qty)\n",
    "\n",
    "    price = rec.get(\"px\")\n",
    "    if price is None and rec.get(\"order\"):\n",
    "        price = rec[\"order\"].get(\"price\")\n",
    "    price = to_float(price)\n",
    "\n",
    "    venue = rec.get(\"venue\") or rec.get(\"mic\")\n",
    "    cp = rec.get(\"cp\") or rec.get(\"counterparty\") or {}\n",
    "    cp_lei = cp.get(\"lei\") if isinstance(cp, dict) else None\n",
    "\n",
    "    if not trade_id:\n",
    "        errors.append(\"Missing trade_id\")\n",
    "    if not execution_time:\n",
    "        errors.append(\"Could not parse execution_time\")\n",
    "    if not isin:\n",
    "        errors.append(\"Missing ISIN\")\n",
    "    if not side:\n",
    "        errors.append(f\"Invalid side: {side_raw}\")\n",
    "    if qty is None or qty <= 0:\n",
    "        errors.append(f\"Invalid quantity: {qty}\")\n",
    "    if price is None or price <= 0:\n",
    "        errors.append(f\"Invalid price: {price}\")\n",
    "    if not venue:\n",
    "        errors.append(\"Missing venue\")\n",
    "\n",
    "    if errors:\n",
    "        return None, errors\n",
    "\n",
    "    notional = compute_notional(qty, price)\n",
    "\n",
    "    canonical = {\n",
    "        \"trade_id\": f\"INT_{trade_id}\",\n",
    "        \"execution_time\": execution_time,\n",
    "        \"instrument\": {\n",
    "            \"isin\": isin,\n",
    "            \"symbol\": symbol,\n",
    "            \"asset_class\": \"EQUITY\"\n",
    "        },\n",
    "        \"side\": side,\n",
    "        \"quantity\": qty,\n",
    "        \"price\": price,\n",
    "        \"notional\": notional,\n",
    "        \"venue_mic\": venue,\n",
    "        \"trader_id\": \"TRDR_UNKNOWN\",\n",
    "        \"account_id\": \"ACC_UNKNOWN\",\n",
    "        \"counterparty_lei\": cp_lei,\n",
    "        \"currency\": \"USD\",\n",
    "        \"order_type\": \"LIMIT\" if price is not None else \"MARKET\",\n",
    "        \"trading_capacity\": \"DEAL\",\n",
    "        \"short_sell_flag\": \"N\"\n",
    "    }\n",
    "\n",
    "    return canonical, []\n",
    "\n",
    "\n",
    "# ---------- Top-level functions ----------\n",
    "\n",
    "def normalize_csv_file(path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Normalize CSV file (using OMS/Broker-style columns).\n",
    "    Returns (list of canonical trades, list of error records).\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    errors = []\n",
    "    with open(path, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            canon, err_list = normalize_row_oms_csv(row)\n",
    "            if canon:\n",
    "                normalized.append(canon)\n",
    "            else:\n",
    "                errors.append({\"raw\": row, \"errors\": err_list})\n",
    "    return normalized, errors\n",
    "\n",
    "\n",
    "def normalize_json_file(path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Normalize JSON file in raw_large_feed_B.json structure.\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    errors = []\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    records = data.get(\"trade_feed\") or data.get(\"executions\") or data\n",
    "    if isinstance(records, dict):\n",
    "        # in case it's under another key\n",
    "        records = records.get(\"trades\", [])\n",
    "\n",
    "    for rec in records:\n",
    "        canon, err_list = normalize_record_json_batch(rec)\n",
    "        if canon:\n",
    "            normalized.append(canon)\n",
    "        else:\n",
    "            errors.append({\"raw\": rec, \"errors\": err_list})\n",
    "    return normalized, errors\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "\n",
    "    csv_trades, csv_errors = normalize_csv_file(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\raw_large_feed_B.csv\")\n",
    "    print(f\"CSV normalized trades: {len(csv_trades)}, errors: {len(csv_errors)}\")\n",
    "\n",
    "    json_trades, json_errors = normalize_json_file(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\raw_large_feed_B.json\")\n",
    "    print(f\"JSON normalized trades: {len(json_trades)}, errors: {len(json_errors)}\")\n",
    "\n",
    "    # Look at one normalized trade\n",
    "    print(\"\\nSample normalized CSV trade:\")\n",
    "    from pprint import pprint\n",
    "    pprint(csv_trades[0])\n",
    "\n",
    "    print(\"\\nSample normalized JSON trade:\")\n",
    "    pprint(json_trades[0])\n",
    "\n",
    "    # Look at a couple of error records\n",
    "    print(\"\\nSample CSV errors:\")\n",
    "    pprint(csv_errors[:2])\n",
    "\n",
    "    print(\"\\nSample JSON errors:\")\n",
    "    pprint(json_errors[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "124bb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 30 normalized trades to canonical_trades_normalized_phase2.json\n",
      "Saved 7 error records to normalization_errors_phase2.json\n"
     ]
    }
   ],
   "source": [
    "    # Combine all normalized trades from CSV + JSON\n",
    "all_trades = csv_trades + json_trades\n",
    "\n",
    "    # Save to JSON for analysis\n",
    "import json\n",
    "with open(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\GenAI\\\\VoyagersRegionalGenAI\\\\savedFiles\\\\canonical_trades_normalized_phase2.json\", \"w\") as f:\n",
    "    json.dump(all_trades, f, indent=2)\n",
    "\n",
    "    # Save errors as well\n",
    "all_errors = csv_errors + json_errors\n",
    "with open(\"c:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\GenAI\\\\VoyagersRegionalGenAI\\\\savedFiles\\\\normalization_errors_phase2.json\", \"w\") as f:\n",
    "    json.dump(all_errors, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved {len(all_trades)} normalized trades to canonical_trades_normalized_phase2.json\")\n",
    "print(f\"Saved {len(all_errors)} error records to normalization_errors_phase2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc245b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    trade_id        execution_time  \\\n",
      "0  INT_A1001  2024-01-25T10:32:09Z   \n",
      "1  INT_A1002  2024-01-25T10:35:42Z   \n",
      "2  INT_A1003  2024-01-25T10:40:12Z   \n",
      "3  INT_A1004  2024-01-25T10:41:01Z   \n",
      "4  INT_A1006  2024-01-25T10:50:09Z   \n",
      "\n",
      "                                          instrument  side  quantity   price  \\\n",
      "0  {'isin': 'US1234567890', 'symbol': 'STK001', '...   BUY      1500   42.35   \n",
      "1  {'isin': 'US9876543210', 'symbol': 'STK007', '...  SELL       820  118.50   \n",
      "2  {'isin': 'US4433559922', 'symbol': 'STK009', '...   BUY      3400   55.20   \n",
      "3  {'isin': 'US0000000000', 'symbol': 'STK011', '...   BUY      1200  220.55   \n",
      "4  {'isin': 'US5647382910', 'symbol': 'STK015', '...  SELL       300  112.10   \n",
      "\n",
      "   notional venue_mic     trader_id   account_id      counterparty_lei  \\\n",
      "0   63525.0      XNAS  TRDR_UNKNOWN  ACC_UNKNOWN  5493001KJTIIGC8Y1R12   \n",
      "1   97170.0      XNYS  TRDR_UNKNOWN  ACC_UNKNOWN  4RRT9021KJTIIGC9PU77   \n",
      "2  187680.0      BATS  TRDR_UNKNOWN  ACC_UNKNOWN  84HD9K21KJT99H33UQ90   \n",
      "3  264660.0      XNAS  TRDR_UNKNOWN  ACC_UNKNOWN  9888KJTCC99PPQ12RT88   \n",
      "4   33630.0      XNYS  TRDR_UNKNOWN  ACC_UNKNOWN                  None   \n",
      "\n",
      "  currency order_type trading_capacity short_sell_flag  \n",
      "0      USD      LIMIT             DEAL               N  \n",
      "1      USD      LIMIT             DEAL               N  \n",
      "2      USD      LIMIT             DEAL               N  \n",
      "3      USD      LIMIT             DEAL               N  \n",
      "4      USD      LIMIT             DEAL               N  \n",
      "venue_mic\n",
      "XNAS    9\n",
      "XNYS    8\n",
      "BATS    5\n",
      "ARCX    3\n",
      "XNES    3\n",
      "XNSE    2\n",
      "Name: count, dtype: int64\n",
      "side\n",
      "BUY     15\n",
      "SELL    15\n",
      "Name: count, dtype: int64\n",
      "            quantity       price      notional\n",
      "count      30.000000   30.000000  3.000000e+01\n",
      "mean    39491.466667  145.545333  1.879564e+05\n",
      "std    183433.459470  111.882439  3.774362e+05\n",
      "min       100.000000    0.010000  1.500000e+03\n",
      "25%       562.500000   57.950000  5.918318e+04\n",
      "50%       980.000000  120.525000  9.679750e+04\n",
      "75%      1875.000000  197.025000  1.530960e+05\n",
      "max    999999.000000  455.000000  2.077750e+06\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\canonical_trades_normalized_phase2.json\") as f:\n",
    "    trades = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(trades)\n",
    "print(df.head())\n",
    "\n",
    "print(df[\"venue_mic\"].value_counts())\n",
    "print(df[\"side\"].value_counts())\n",
    "print(df[[\"quantity\", \"price\", \"notional\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a775d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 canonical trades\n",
      "Valid regulatory records: 500\n",
      "Invalid / failed records: 0\n",
      "Saved regulatory_report_records.json and regulatory_validation_errors.json\n"
     ]
    }
   ],
   "source": [
    "#regulation_mapping_agent.py3\n",
    "\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "\n",
    "def is_valid_isin(isin: str) -> bool:\n",
    "    \"\"\"\n",
    "    Very simple ISIN check for demo:\n",
    "    - length 12\n",
    "    - first 2 are letters\n",
    "    - remaining 10 are digits\n",
    "\n",
    "    (Real ISIN check is more complex with a checksum,\n",
    "     but this is enough for hackathon demo.)\n",
    "    \"\"\"\n",
    "    if not isin or len(isin) != 12:\n",
    "        return False\n",
    "    if not isin[:2].isalpha():\n",
    "        return False\n",
    "    if not isin[2:].isdigit():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_valid_lei(lei: str) -> bool:\n",
    "    \"\"\"\n",
    "    Simple LEI validation for demo:\n",
    "    - length 20\n",
    "    - alphanumeric\n",
    "    \"\"\"\n",
    "    if not lei:\n",
    "        return False\n",
    "    if len(lei) != 20:\n",
    "        return False\n",
    "    return lei.isalnum()\n",
    "\n",
    "\n",
    "def get_nested_value(obj: Dict[str, Any], path: str):\n",
    "    \"\"\"\n",
    "    Resolve dot-path like 'instrument.isin' from the canonical trade object.\n",
    "    \"\"\"\n",
    "    parts = path.split(\".\")\n",
    "    cur = obj\n",
    "    for p in parts:\n",
    "        if isinstance(cur, dict) and p in cur:\n",
    "            cur = cur[p]\n",
    "        else:\n",
    "            return None\n",
    "    return cur\n",
    "\n",
    "\n",
    "# ----------------- Core Mapping & Validation -----------------\n",
    "\n",
    "def map_trade_to_report(\n",
    "    trade: Dict[str, Any],\n",
    "    template: Dict[str, Any]\n",
    ") -> Tuple[Dict[str, Any], List[str]]:\n",
    "    \"\"\"\n",
    "    Map a canonical trade to a regulatory report record\n",
    "    using the given template. Return (report_record, errors).\n",
    "    \"\"\"\n",
    "    errors: List[str] = []\n",
    "    report_record: Dict[str, Any] = {}\n",
    "\n",
    "    # 1) Field mapping\n",
    "    for field_spec in template[\"fields_required\"]:\n",
    "        name = field_spec[\"name\"]\n",
    "        source = field_spec[\"source\"]\n",
    "        fmt = field_spec.get(\"format\")\n",
    "\n",
    "        value = get_nested_value(trade, source)\n",
    "\n",
    "        # Basic presence check\n",
    "        if value is None or value == \"\":\n",
    "            errors.append(f\"Missing required field: {name} (source: {source})\")\n",
    "        else:\n",
    "            # Type / format specific checks\n",
    "            if fmt == \"integer\":\n",
    "                try:\n",
    "                    value = int(value)\n",
    "                except Exception:\n",
    "                    errors.append(f\"Invalid integer for {name}: {value}\")\n",
    "            elif fmt == \"decimal\":\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except Exception:\n",
    "                    errors.append(f\"Invalid decimal for {name}: {value}\")\n",
    "            # you can add more types if needed\n",
    "\n",
    "        report_record[name] = value\n",
    "\n",
    "    # 2) Extra validation rules from template\n",
    "    # (We implement them explicitly rather than parsing the strings)\n",
    "    q = report_record.get(\"Quantity\")\n",
    "    p = report_record.get(\"Price\")\n",
    "    isin = report_record.get(\"InstrumentIdentificationCode\")\n",
    "    lei = report_record.get(\"BuyerLei\")\n",
    "\n",
    "    if q is None or q <= 0:\n",
    "        errors.append(f\"Validation: Quantity must be > 0, got {q}\")\n",
    "    if p is None or p <= 0:\n",
    "        errors.append(f\"Validation: Price must be > 0, got {p}\")\n",
    "    if not isin or not isinstance(isin, str) or not is_valid_isin(isin):\n",
    "        errors.append(f\"Validation: Invalid ISIN: {isin}\")\n",
    "    if not lei or not isinstance(lei, str) or not is_valid_lei(lei):\n",
    "        errors.append(f\"Validation: Invalid LEI: {lei}\")\n",
    "\n",
    "    return report_record, errors\n",
    "\n",
    "\n",
    "def process_trades_for_regulation(\n",
    "    trades: List[Dict[str, Any]],\n",
    "    template: Dict[str, Any]\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    For each canonical trade, produce either:\n",
    "    - a valid regulatory record, or\n",
    "    - an error record with reasons.\n",
    "\n",
    "    Returns (valid_records, error_records).\n",
    "    \"\"\"\n",
    "    valid_records: List[Dict[str, Any]] = []\n",
    "    error_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for trade in trades:\n",
    "        report_record, errors = map_trade_to_report(trade, template)\n",
    "        if errors:\n",
    "            error_records.append({\n",
    "                \"trade_id\": trade.get(\"trade_id\"),\n",
    "                \"raw_trade\": trade,\n",
    "                \"errors\": errors\n",
    "            })\n",
    "        else:\n",
    "            valid_records.append({\n",
    "                \"trade_id\": trade.get(\"trade_id\"),\n",
    "                \"report_record\": report_record\n",
    "            })\n",
    "\n",
    "    return valid_records, error_records\n",
    "\n",
    "\n",
    "# ----------------- Main -----------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load canonical trades (output of Phase 2 or Phase 1)\n",
    "    with open(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\canonical_trades_phase1_500.json\") as f:\n",
    "        canonical_trades = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(canonical_trades)} canonical trades\")\n",
    "\n",
    "    # 2) Load regulation template\n",
    "    with open(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulation_template_phase1.json\") as f:\n",
    "        template = json.load(f)\n",
    "\n",
    "    # 3) Process\n",
    "    valid_records, invalid_records = process_trades_for_regulation(canonical_trades, template)\n",
    "\n",
    "    print(f\"Valid regulatory records: {len(valid_records)}\")\n",
    "    print(f\"Invalid / failed records: {len(invalid_records)}\")\n",
    "\n",
    "    # 4) Save outputs\n",
    "    with open(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_report_records.json\", \"w\") as f:\n",
    "        json.dump(valid_records, f, indent=2)\n",
    "\n",
    "    with open(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_validation_errors.json\", \"w\") as f:\n",
    "        json.dump(invalid_records, f, indent=2)\n",
    "\n",
    "    print(\"Saved regulatory_report_records.json and regulatory_validation_errors.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trade_id       TradingDateTime InstrumentIdentificationCode   Price  \\\n",
      "0  INT_100000  2024-01-11T23:15:00Z                 US7469267437   95.00   \n",
      "1  INT_100001  2024-01-29T11:14:00Z                 US0317315593  484.94   \n",
      "2  INT_100002  2024-01-02T20:55:00Z                 US2072204969  168.79   \n",
      "3  INT_100003  2024-01-05T10:14:00Z                 US3625198453  271.97   \n",
      "4  INT_100004  2024-01-01T14:14:00Z                 US0021495636  187.05   \n",
      "\n",
      "   Quantity VenueMIC              BuyerLei  \n",
      "0        92     XNSE  VY01POCGUIRDYNIZBT1G  \n",
      "1       255     XNYS  J9N4OHNWUPRXZY2OAYGI  \n",
      "2       206     XNSE  V08A8NCAEZN0IOOZM55D  \n",
      "3       393     XNYS  6A0MCFH0I6XO55XLBCQB  \n",
      "4       327     ARCX  VKP6IFP2CT9SZCST5G6R  \n",
      "\n",
      "Counts by VenueMIC:\n",
      "VenueMIC\n",
      "XNSE    107\n",
      "BATS    106\n",
      "XNYS    105\n",
      "ARCX     94\n",
      "XNAS     88\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Side-by-side stats:\n",
      "            Price     Quantity\n",
      "count  500.000000   500.000000\n",
      "mean   246.954120   252.008000\n",
      "std    146.960486   329.124764\n",
      "min      5.540000    12.000000\n",
      "25%    118.840000    75.750000\n",
      "50%    234.440000   147.000000\n",
      "75%    380.507500   292.250000\n",
      "max    499.510000  3673.000000\n"
     ]
    }
   ],
   "source": [
    "#regulatory_report_records.json\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load valid regulatory records\n",
    "with open(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_report_records.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten structure: each entry has { trade_id, report_record }\n",
    "rows = []\n",
    "for rec in data:\n",
    "    row = {\"trade_id\": rec[\"trade_id\"]}\n",
    "    row.update(rec[\"report_record\"])\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nCounts by VenueMIC:\")\n",
    "print(df[\"VenueMIC\"].value_counts())\n",
    "\n",
    "print(\"\\nSide-by-side stats:\")\n",
    "print(df[[\"Price\", \"Quantity\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072416a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 valid regulatory records\n",
      "Loaded 0 invalid records\n",
      "Wrote regulatory_transaction_report.csv\n",
      "Wrote regulatory_report_summary.json and regulatory_report_summary.txt\n"
     ]
    }
   ],
   "source": [
    "#regulatory_report_records.json\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def load_valid_records(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load valid regulatory records from Phase 3 output.\n",
    "    Each record looks like:\n",
    "    {\n",
    "      \"trade_id\": \"...\",\n",
    "      \"report_record\": { ...fields... }\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_validation_errors(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load invalid records from Phase 3 output.\n",
    "    Each record looks like:\n",
    "    {\n",
    "      \"trade_id\": \"...\",\n",
    "      \"raw_trade\": { ...canonical... },\n",
    "      \"errors\": [ ... ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def write_regulator_csv(valid_records: List[Dict[str, Any]], path: str):\n",
    "    \"\"\"\n",
    "    Generate final regulator-facing CSV file.\n",
    "    \"\"\"\n",
    "    # Define output order of columns\n",
    "    fieldnames = [\n",
    "        \"trade_id\",\n",
    "        \"TradingDateTime\",\n",
    "        \"InstrumentIdentificationCode\",\n",
    "        \"Price\",\n",
    "        \"Quantity\",\n",
    "        \"VenueMIC\",\n",
    "        \"BuyerLei\"\n",
    "    ]\n",
    "\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for rec in valid_records:\n",
    "            tid = rec.get(\"trade_id\")\n",
    "            rr = rec.get(\"report_record\", {})\n",
    "\n",
    "            row = {\n",
    "                \"trade_id\": tid,\n",
    "                \"TradingDateTime\": rr.get(\"TradingDateTime\"),\n",
    "                \"InstrumentIdentificationCode\": rr.get(\"InstrumentIdentificationCode\"),\n",
    "                \"Price\": rr.get(\"Price\"),\n",
    "                \"Quantity\": rr.get(\"Quantity\"),\n",
    "                \"VenueMIC\": rr.get(\"VenueMIC\"),\n",
    "                \"BuyerLei\": rr.get(\"BuyerLei\")\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def build_summary(\n",
    "    valid_records: List[Dict[str, Any]],\n",
    "    invalid_records: List[Dict[str, Any]]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a high-level summary for compliance / reporting.\n",
    "    \"\"\"\n",
    "    total = len(valid_records) + len(invalid_records)\n",
    "    valid = len(valid_records)\n",
    "    invalid = len(invalid_records)\n",
    "\n",
    "    # Count errors by type\n",
    "    error_counter = Counter()\n",
    "    for err_rec in invalid_records:\n",
    "        for e in err_rec.get(\"errors\", []):\n",
    "            error_counter[e] += 1\n",
    "\n",
    "    # Basic venue stats\n",
    "    venue_counter = Counter()\n",
    "    for rec in valid_records:\n",
    "        rr = rec.get(\"report_record\", {})\n",
    "        venue = rr.get(\"VenueMIC\")\n",
    "        if venue:\n",
    "            venue_counter[venue] += 1\n",
    "\n",
    "    summary = {\n",
    "        \"total_trades_processed\": total,\n",
    "        \"valid_regulatory_records\": valid,\n",
    "        \"invalid_records\": invalid,\n",
    "        \"valid_ratio\": round(valid / total, 4) if total > 0 else None,\n",
    "        \"invalid_ratio\": round(invalid / total, 4) if total > 0 else None,\n",
    "        \"top_error_types\": error_counter.most_common(10),\n",
    "        \"venue_distribution\": venue_counter.most_common(),\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def write_summary_files(summary: Dict[str, Any], path_json: str, path_txt: str):\n",
    "    \"\"\"\n",
    "    Save summary in JSON and a human-readable text version.\n",
    "    \"\"\"\n",
    "    # JSON\n",
    "    with open(path_json, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # Text\n",
    "    lines = []\n",
    "    lines.append(\"Compliance Reporting Summary\")\n",
    "    lines.append(\"============================\")\n",
    "    lines.append(f\"Total trades processed: {summary['total_trades_processed']}\")\n",
    "    lines.append(f\"Valid regulatory records: {summary['valid_regulatory_records']}\")\n",
    "    lines.append(f\"Invalid records: {summary['invalid_records']}\")\n",
    "    lines.append(f\"Valid ratio: {summary['valid_ratio']}\")\n",
    "    lines.append(f\"Invalid ratio: {summary['invalid_ratio']}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Top error types:\")\n",
    "    if summary[\"top_error_types\"]:\n",
    "        for err_msg, count in summary[\"top_error_types\"]:\n",
    "            lines.append(f\"  - {err_msg}  (count={count})\")\n",
    "    else:\n",
    "        lines.append(\"  (No errors)\")\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Venue distribution (valid records):\")\n",
    "    if summary[\"venue_distribution\"]:\n",
    "        for venue, count in summary[\"venue_distribution\"]:\n",
    "            lines.append(f\"  - {venue}: {count}\")\n",
    "    else:\n",
    "        lines.append(\"  (No valid records)\")\n",
    "\n",
    "    with open(path_txt, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load Phase 3 outputs\n",
    "    valid_records = load_valid_records(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_report_records.json\")\n",
    "    invalid_records = load_validation_errors(r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_validation_errors.json\")\n",
    "\n",
    "    print(f\"Loaded {len(valid_records)} valid regulatory records\")\n",
    "    print(f\"Loaded {len(invalid_records)} invalid records\")\n",
    "\n",
    "    # 2) Generate regulator CSV\n",
    "    write_regulator_csv(valid_records, r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_transaction_report.csv\")\n",
    "    print(\"Wrote regulatory_transaction_report.csv\")\n",
    "\n",
    "    # 3) Build summary\n",
    "    summary = build_summary(valid_records, invalid_records)\n",
    "\n",
    "    # 4) Save summary in JSON + TXT\n",
    "    write_summary_files(summary, r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_report_summary.json\", r\"c:\\Users\\Lenovo\\OneDrive\\Desktop\\GenAI\\VoyagersRegionalGenAI\\savedFiles\\regulatory_report_summary.txt\")\n",
    "    print(\"Wrote regulatory_report_summary.json and regulatory_report_summary.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VoyagersRegionalGenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
