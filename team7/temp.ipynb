{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc185946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae0705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import httpx\n",
    "\n",
    "# Create an HTTP client that skips SSL verification (only for hackathon/test environments)\n",
    "client = httpx.Client(verify=False)\n",
    "llm = ChatOpenAI(\n",
    " base_url=\"https://genailab.tcs.in\",\n",
    " model=\"azure/genailab-maas-gpt-4o\",\n",
    " api_key=\"sk-u6zTQaiDKlhHn4-k_hhihw\",\n",
    " http_client=client\n",
    ")\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(\n",
    " base_url=\"https://genailab.tcs.in\",\n",
    " model=\"azure/genailab-maas-text-embedding-3-large\",\n",
    " api_key=\"sk-u6zTQaiDKlhHn4-k_hhihw\",\n",
    " http_client=client)\n",
    "\n",
    "import requests\n",
    "for method in (\"get\",\"post\",\"put\",\"delete\",\"head\",\"options\",\"patch\"):\n",
    "    original = getattr(requests,method)\n",
    "\n",
    "    def insecure_request(*args, _original = original, **kwargs):\n",
    "        kwargs[\"verify\"] = False\n",
    "        return _original(*args,**kwargs)\n",
    "    \n",
    "    setattr(requests,method,insecure_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28b68198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Triage Level**: Medium  \n",
      "- **Reasoning**: The symptoms of increased thirst and urination could indicate poor blood sugar control, which is concerning given the patient's history of Type 2 Diabetes. These symptoms may suggest hyperglycemia (high blood sugar levels) or potential complications such as undiagnosed diabetes progression, which requires prompt medical evaluation.  \n",
      "- **Urgent Evaluation Needed**: Yes; the patient should have their blood sugar levels checked immediately through fasting glucose, hemoglobin A1c, or random glucose testing. Urinalysis to check for glucose or ketones in the urine may also be considered.  \n",
      "- **Patient Actions**:  \n",
      "  1. Contact your primary care provider immediately to report symptoms.  \n",
      "  2. Monitor blood sugar levels if you have a glucometer and share results with your clinician.  \n",
      "  3. Maintain proper hydration and avoid sugary beverages until seen by a healthcare provider.  \n",
      "  4. Do not delay seeking care, especially if symptoms worsen or you experience additional signs like nausea, vomiting, or abdominal pain.  \n",
      "- **Clinician Tasks**:  \n",
      "  1. Assess the patient’s current blood sugar control through lab tests (e.g., fasting glucose, HbA1c).  \n",
      "  2. Adjust diabetes medications if needed to control blood sugar levels.  \n",
      "  3. Evaluate for possible diabetes complications such as diabetic ketoacidosis (if suspected) or worsening nephropathy.  \n",
      "  4. Provide education on proper diabetes management strategies and symptom monitoring.  \n",
      "- **Disclaimer**: This response is for informational purposes only and is not a substitute for professional medical advice, diagnosis, or treatment. Please consult your healthcare provider for personalized guidance.  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Loader: read all .txt files\n",
    "# -------------------------------\n",
    "def load_txts():\n",
    "    txt_folder = r\"C:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\database\\txt_files\"\n",
    "    txt_files = [os.path.join(txt_folder, f) for f in os.listdir(txt_folder) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "    all_docs = []\n",
    "    for txt_file in txt_files:\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            # Return LangChain Document objects\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": os.path.basename(txt_file)}\n",
    "            )\n",
    "            all_docs.append(doc)\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Loader: read JSON EHR file\n",
    "# -------------------------------\n",
    "def load_jsons():\n",
    "    file_path = r\"C:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\database\\json_files\\mock_ehr.json\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ehr_Report = json.load(f)\n",
    "    return ehr_Report\n",
    "\n",
    "def get_patient_by_id(patient_id):\n",
    "    ehr_data = load_jsons()\n",
    "    for patient in ehr_data.get(\"patients\", []):\n",
    "        if patient.get(\"id\") == patient_id:\n",
    "            return patient\n",
    "    return None \n",
    "\n",
    "patient_record_json = get_patient_by_id(\"P001\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Helper: convert JSON to text\n",
    "# -------------------------------\n",
    "def json_to_text(d, prefix=\"- \"):\n",
    "    lines = []\n",
    "    for key, value in d.items():\n",
    "        key_name = key.replace(\"_\", \" \").title()\n",
    "        if isinstance(value, dict):\n",
    "            lines.append(f\"{prefix}{key_name}:\")\n",
    "            lines.extend([f\"  {line}\" for line in json_to_text(value, prefix=prefix)])\n",
    "        elif isinstance(value, list):\n",
    "            if all(isinstance(i, dict) for i in value):\n",
    "                lines.append(f\"{prefix}{key_name}:\")\n",
    "                for item in value:\n",
    "                    lines.extend([f\"  {line}\" for line in json_to_text(item, prefix=prefix)])\n",
    "            else:\n",
    "                lines.append(f\"{prefix}{key_name}: {', '.join(map(str,value))}\")\n",
    "        else:\n",
    "            lines.append(f\"{prefix}{key_name}: {value}\")\n",
    "    return lines\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Retriever builder\n",
    "# -------------------------------\n",
    "def getRetriever():\n",
    "    txt_documents = load_txts()\n",
    "    chunked_docs = []\n",
    "    for doc in txt_documents:\n",
    "        # Prompt template for chunking\n",
    "        chunk_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \n",
    "                \"You are a helpful assistant that splits medical or disease-related documents \"\n",
    "                \"into semantically meaningful chunks for downstream semantic search and retrieval.\"\n",
    "            ),\n",
    "            (\"human\", \n",
    "                \"\"\"You will be given a document. Split it into semantically coherent sections, following these rules:\n",
    "\n",
    "                1. Do not omit any information.\n",
    "                2. Keep related items together.\n",
    "                3. Each chunk should be self-contained.\n",
    "                4. Aim for ~5000–6000 words per chunk more or less.\n",
    "                5. Return chunks as a numbered list with headings like '### Chunk 1:', '### Chunk 2:'.\n",
    "                6. Include metadata (e.g., source) at the beginning of each chunk.\n",
    "\n",
    "                Document:\n",
    "                {document}\n",
    "                \"\"\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        formatted_prompt = chunk_prompt.format_messages(document=doc.page_content)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "\n",
    "        # Split on \"### Chunk\" headings\n",
    "        chunks = [c.strip() for c in response.content.split(\"### Chunk\") if c.strip()]\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunked_docs.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": f\"{doc.metadata['source']}_chunk{idx+1}\"}\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Build vector store\n",
    "    vector_store = Chroma.from_documents(\n",
    "        chunked_docs,\n",
    "        embedding_model,\n",
    "        persist_directory=\"chroma_db\"\n",
    "    )\n",
    "\n",
    "    # Prepare patient record text\n",
    "    patient_record = \"Patient Past Record:\\n\" + \"\\n\".join(json_to_text(patient_record_json)) \\\n",
    "        if patient_record_json else \"No past record found.\"\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # Prompt for answering queries\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "            \"You are a helpful medical assistant. Answer questions using only the provided context. \"\n",
    "            \"If the answer is not in the context, say you don't know rather than making up information.\"\n",
    "        ),\n",
    "        (\"human\",\n",
    "            \"\"\"You are given the patient's past medical records and the retrieved context from relevant documents.\n",
    "\n",
    "            Context (retrieved from vector database):\n",
    "            {context}\n",
    "\n",
    "            Patient Past Record:\n",
    "            {patient_record}\n",
    "\n",
    "            Current patient query / symptom:\n",
    "            {question}\n",
    "\n",
    "            Guidelines:\n",
    "            1. Use ONLY the information from the retrieved context and patient past record.\n",
    "            2. Synthesize both sources to provide a clear, concise response.\n",
    "            3. Do NOT invent facts not present in the context or past record.\n",
    "            4. Provide your response in the following structured format:\n",
    "\n",
    "                - **Triage Level**: [Low / Medium / High / Emergency]\n",
    "                - **Reasoning**: [Explain why this triage level is assigned]\n",
    "                - **Urgent Evaluation Needed**: [Yes/No; specify tests if any]\n",
    "                - **Patient Actions**: [What patient should do next]\n",
    "                - **Clinician Tasks**: [Tasks clinician should perform]\n",
    "                - **Disclaimer**: [Include standard medical disclaimer]\n",
    "\n",
    "            5. If the context and past record do not contain the answer, respond:\n",
    "                \"The provided documents do not contain information about this.\"\n",
    "            \"\"\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Build RAG chain with patient_record injected\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"patient_record\": lambda _: patient_record,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Run a query\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"I'm very thirsty and peeing a lot, even at night.\"\n",
    "    rag_chain = getRetriever()\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a794fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_txts():\n",
    "#     txt_folder = r\"C:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\database\\txt_files\"\n",
    "#     txt_files = [os.path.join(txt_folder, f) for f in os.listdir(txt_folder) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "#     all_docs = []\n",
    "\n",
    "#     for txt_file in txt_files:\n",
    "#         with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             content = f.read()\n",
    "\n",
    "#             # Create a simple document-like dict (similar to PDF loader output)\n",
    "#             doc = {\n",
    "#                 \"page_content\": content,\n",
    "#                 \"metadata\": {\"source\": os.path.basename(txt_file)}\n",
    "#             }\n",
    "#             all_docs.append(doc)\n",
    "\n",
    "#      # Combine all text into one big string\n",
    "#     full_text = \"\\n\".join([doc[\"page_content\"] for doc in all_docs])\n",
    "#     return full_text\n",
    "#     # return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.runnables import RunnableMap\n",
    "# from loader import load_txts\n",
    "# from azure_llm import getMassGpt\n",
    "# from embedding import getLargeEmbedding\n",
    "\n",
    "# def getRetriever():\n",
    "#     txt_documents = load_txts()\n",
    "\n",
    "#     # combined_texts = [\n",
    "#     #     {\"text\": full_text1, \"source_id\": \"PDF_Doc1\"},\n",
    "#     #     {\"text\": full_text2, \"source_id\": \"TXT_Doc2\"}\n",
    "#     # ]\n",
    "\n",
    "#     llm = getMassGpt()\n",
    "#     embedding_model = getLargeEmbedding()\n",
    "\n",
    "#     chunked_docs = []\n",
    "#     for doc in txt_documents:\n",
    "#         # Use your existing chunking prompt\n",
    "#         prompt = ChatPromptTemplate.from_messages([\n",
    "#             (\"system\", \n",
    "#                 \"You are a helpful assistant that splits medical or disease-related documents \"\n",
    "#                 \"into semantically meaningful chunks for downstream semantic search and retrieval.\"\n",
    "#             ),\n",
    "#             (\"human\", \n",
    "#                 \"\"\"You will be given a document. Split it into semantically coherent sections, following these rules:\n",
    "\n",
    "#                 1. **Completeness**: Do not omit any information — every detail must appear in some chunk.\n",
    "#                 2. **Semantic grouping**: Keep related items (like lists of symptoms, causes, treatments, dates, or tables) together in the same chunk.\n",
    "#                 3. **Self-contained**: Each chunk should be understandable on its own without needing other chunks.\n",
    "#                 4. **Chunk size**: Aim for ~5000–6000 words per chunk. If the document is shorter, keep it intact. \n",
    "#                     If longer, split carefully without breaking sentences, lists, or tables.\n",
    "#                 5. **Formatting**: Return the chunks as a numbered list. Clearly separate each chunk with a heading like 'Chunk 1:', 'Chunk 2:', etc.\n",
    "#                 6. **Metadata preservation**: If the document contains metadata (e.g., disease name, category, source), \n",
    "#                     include it at the beginning of the relevant chunk.\n",
    "\n",
    "#                 Document:\n",
    "#                 {document}\n",
    "#                 \"\"\"\n",
    "#             )\n",
    "#         ])\n",
    "\n",
    "#         formatted_prompt = prompt.format_messages(document=doc['page_content'])\n",
    "#         response = llm.invoke(formatted_prompt)\n",
    "#         chunks = response.content.split(\"\\n\\n\")\n",
    "        \n",
    "#         for idx, chunk in enumerate(chunks):\n",
    "#             if chunk.strip():\n",
    "#                 chunked_docs.append(\n",
    "#                     Document(\n",
    "#                         page_content=chunk.strip(),\n",
    "#                         metadata={\"source_id\": f\"{doc['source_id']}_chunk{idx+1}\"}\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "#     vector_store = Chroma.from_documents(\n",
    "#         chunked_docs,\n",
    "#         embedding_model,\n",
    "#         persist_directory=\"chroma_db\"\n",
    "#     )\n",
    "\n",
    "#     retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "#     rag_chain = (\n",
    "#         {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#     )\n",
    "#     return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Forecast the next 12 months of sales for Laptops\"\n",
    "# rag_chain = getRetriever()\n",
    "# response = rag_chain.invoke(query)\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd67a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def json_to_text(d, prefix=\"- \"):\n",
    "#     lines = []\n",
    "#     for key, value in d.items():\n",
    "#         key_name = key.replace(\"_\", \" \").title()\n",
    "#         if isinstance(value, dict):\n",
    "#             lines.append(f\"{prefix}{key_name}:\")\n",
    "#             lines.extend([f\"  {line}\" for line in json_to_text(value, prefix=prefix)])\n",
    "#         elif isinstance(value, list):\n",
    "#             if all(isinstance(i, dict) for i in value):\n",
    "#                 lines.append(f\"{prefix}{key_name}:\")\n",
    "#                 for item in value:\n",
    "#                     lines.extend([f\"  {line}\" for line in json_to_text(item, prefix=prefix)])\n",
    "#             else:\n",
    "#                 lines.append(f\"{prefix}{key_name}: {', '.join(map(str,value))}\")\n",
    "#         else:\n",
    "#             lines.append(f\"{prefix}{key_name}: {value}\")\n",
    "#     return lines\n",
    "\n",
    "# formatted_record = \"Patient Past Record:\\n\" + \"\\n\".join(json_to_text(patient_record))\n",
    "# print(formatted_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# # -------------------------------\n",
    "# # 1. Loader: read all .txt files\n",
    "# # -------------------------------\n",
    "# def load_txts():\n",
    "#     txt_folder = r\"C:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\database\\txt_files\"\n",
    "#     txt_files = [os.path.join(txt_folder, f) for f in os.listdir(txt_folder) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "#     all_docs = []\n",
    "#     for txt_file in txt_files:\n",
    "#         with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             content = f.read()\n",
    "#             # Return LangChain Document objects\n",
    "#             doc = Document(\n",
    "#                 page_content=content,\n",
    "#                 metadata={\"source\": os.path.basename(txt_file)}\n",
    "#             )\n",
    "#             all_docs.append(doc)\n",
    "\n",
    "#     return all_docs\n",
    "\n",
    "# def load_jsons():\n",
    "#     file_path = r\"C:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\database\\json_files\\mock_ehr.json\"\n",
    "#     # Open and load JSON\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         ehr_Report = json.load(f)\n",
    "#     return ehr_Report\n",
    "\n",
    "# def get_patient_by_id(patient_id):\n",
    "#     ehr_data = load_jsons()\n",
    "#     for patient in ehr_data.get(\"patients\", []):\n",
    "#         if patient.get(\"id\") == patient_id:\n",
    "#             return patient\n",
    "#     return None \n",
    "\n",
    "\n",
    "# patient_record_json = get_patient_by_id(\"P001\")\n",
    "\n",
    "# def json_to_text(d, prefix=\"- \"):\n",
    "#     lines = []\n",
    "#     for key, value in d.items():\n",
    "#         key_name = key.replace(\"_\", \" \").title()\n",
    "#         if isinstance(value, dict):\n",
    "#             lines.append(f\"{prefix}{key_name}:\")\n",
    "#             lines.extend([f\"  {line}\" for line in json_to_text(value, prefix=prefix)])\n",
    "#         elif isinstance(value, list):\n",
    "#             if all(isinstance(i, dict) for i in value):\n",
    "#                 lines.append(f\"{prefix}{key_name}:\")\n",
    "#                 for item in value:\n",
    "#                     lines.extend([f\"  {line}\" for line in json_to_text(item, prefix=prefix)])\n",
    "#             else:\n",
    "#                 lines.append(f\"{prefix}{key_name}: {', '.join(map(str,value))}\")\n",
    "#         else:\n",
    "#             lines.append(f\"{prefix}{key_name}: {value}\")\n",
    "#     return lines\n",
    "\n",
    "# # -------------------------------\n",
    "# # 2. Retriever builder\n",
    "# # -------------------------------\n",
    "# def getRetriever():\n",
    "#     txt_documents = load_txts()\n",
    "#     chunked_docs = []\n",
    "\n",
    "#     for doc in txt_documents:\n",
    "#         # Prompt template\n",
    "#         prompt = ChatPromptTemplate.from_messages([\n",
    "#             (\"system\", \n",
    "#                 \"You are a helpful assistant that splits medical or disease-related documents \"\n",
    "#                 \"into semantically meaningful chunks for downstream semantic search and retrieval.\"\n",
    "#             ),\n",
    "#             (\"human\", \n",
    "#                 \"\"\"You will be given a document. Split it into semantically coherent sections, following these rules:\n",
    "\n",
    "#                 1. Do not omit any information.\n",
    "#                 2. Keep related items together.\n",
    "#                 3. Each chunk should be self-contained.\n",
    "#                 4. Aim for ~5000–6000 words per chunk more or less.\n",
    "#                 5. Return chunks as a numbered list with headings like 'Chunk 1:', 'Chunk 2:'.\n",
    "#                 6. Include metadata (e.g., source) at the beginning of each chunk.\n",
    "\n",
    "#                 Document:\n",
    "#                 {document}\n",
    "#                 \"\"\"\n",
    "#             )\n",
    "#         ])\n",
    "\n",
    "#         # Format prompt with actual document text\n",
    "#         formatted_prompt = prompt.format_messages(document=doc.page_content)\n",
    "#         response = llm.invoke(formatted_prompt)\n",
    "\n",
    "#         # Split on \"Chunk\" headings instead of fragile \\n\\n\n",
    "#         chunks = [c.strip() for c in response.content.split(\"Chunk\") if c.strip()]\n",
    "\n",
    "#         for idx, chunk in enumerate(chunks):\n",
    "#             chunked_docs.append(\n",
    "#                 Document(\n",
    "#                     page_content=chunk,\n",
    "#                     metadata={\"source\": f\"{doc.metadata['source']}_chunk{idx+1}\"}\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     # Build vector store\n",
    "#     vector_store = Chroma.from_documents(\n",
    "#         chunked_docs,\n",
    "#         embedding_model,\n",
    "#         persist_directory=\"chroma_db\"\n",
    "#     )\n",
    "\n",
    "#     patient_record = \"Patient Past Record:\\n\" + \"\\n\".join(json_to_text(patient_record_json))\n",
    "#     retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages([\n",
    "#         (\"system\",\n",
    "#             \"You are a helpful medical assistant. Answer questions using only the provided context. \"\n",
    "#             \"If the answer is not in the context, say you don't know rather than making up information.\"\n",
    "#         ),\n",
    "#         (\"human\",\n",
    "#             \"\"\"You are given the patient's past medical records and the retrieved context from relevant documents.\n",
    "\n",
    "#             Context (retrieved from vector database):\n",
    "#             {context}\n",
    "\n",
    "#             Patient Past Record:\n",
    "#             {patient_record}\n",
    "\n",
    "#             Current patient query / symptom:\n",
    "#             {question}\n",
    "\n",
    "#             Guidelines:\n",
    "#             1. Use ONLY the information from the retrieved context and patient past record.\n",
    "#             2. Synthesize both sources to provide a clear, concise response.\n",
    "#             3. Do NOT invent facts not present in the context or past record.\n",
    "#             4. Provide your response in the following structured format:\n",
    "\n",
    "#                 - **Triage Level**: [Low / Medium / High / Emergency]\n",
    "#                 - **Reasoning**: [Explain why this triage level is assigned]\n",
    "#                 - **Urgent Evaluation Needed**: [Yes/No; specify tests if any]\n",
    "#                 - **Patient Actions**: [What patient should do next]\n",
    "#                 - **Clinician Tasks**: [Tasks clinician should perform]\n",
    "#                 - **Disclaimer**: [Include standard medical disclaimer]\n",
    "\n",
    "#             5. If the context and past record do not contain the answer, respond:\n",
    "#                 \"The provided documents do not contain information about this.\"\n",
    "#             \"\"\"\n",
    "#         )\n",
    "#     ])\n",
    "\n",
    "\n",
    "#     # Build RAG chain\n",
    "#     rag_chain = (\n",
    "#         {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#     )\n",
    "#     return rag_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65a6f2fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to ChatPromptTemplate is missing variables {'patient_record'}.  Expected: ['context', 'patient_record', 'question'] Received: ['context', 'question']\\nNote: if you intended {patient_record} to be part of the string and not a variable, please escape it with double curly braces like: '{{patient_record}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm very thirsty and peeing a lot, even at night.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m rag_chain = getRetriever()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3129\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:217\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    216\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2050\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2048\u001b[39m         output = cast(\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2058\u001b[39m         )\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2060\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:190\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:184\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    178\u001b[39m     example_key = missing.pop()\n\u001b[32m    179\u001b[39m     msg += (\n\u001b[32m    180\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    185\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    186\u001b[39m     )\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to ChatPromptTemplate is missing variables {'patient_record'}.  Expected: ['context', 'patient_record', 'question'] Received: ['context', 'question']\\nNote: if you intended {patient_record} to be part of the string and not a variable, please escape it with double curly braces like: '{{patient_record}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "query = \"I'm very thirsty and peeing a lot, even at night.\"\n",
    "rag_chain = getRetriever()\n",
    "response = rag_chain.invoke(query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# # -------------------------------\n",
    "# # 1. Loader: read all .txt files\n",
    "# # -------------------------------\n",
    "# def load_txts():\n",
    "#     txt_folder = r\"C:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\database\\txt_files\"\n",
    "#     txt_files = [os.path.join(txt_folder, f) for f in os.listdir(txt_folder) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "#     all_docs = []\n",
    "#     for txt_file in txt_files:\n",
    "#         with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             content = f.read()\n",
    "#             # Return LangChain Document objects\n",
    "#             doc = Document(\n",
    "#                 page_content=content,\n",
    "#                 metadata={\"source\": os.path.basename(txt_file)}\n",
    "#             )\n",
    "#             all_docs.append(doc)\n",
    "\n",
    "#     return all_docs\n",
    "\n",
    "# def load_jsons():\n",
    "#     file_path = r\"C:\\Users\\GenAIBLRANCUSR33\\Desktop\\Team7\\database\\json_files\\mock_ehr.json\"\n",
    "#     # Open and load JSON\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         ehr_Report = json.load(f)\n",
    "#     return ehr_Report\n",
    "\n",
    "# def get_patient_by_id(patient_id):\n",
    "#     ehr_data = load_jsons()\n",
    "#     for patient in ehr_data.get(\"patients\", []):\n",
    "#         if patient.get(\"id\") == patient_id:\n",
    "#             return patient\n",
    "#     return None \n",
    "\n",
    "\n",
    "# patient_record_json = get_patient_by_id(\"P001\")\n",
    "\n",
    "# def json_to_text(d, prefix=\"- \"):\n",
    "#     lines = []\n",
    "#     for key, value in d.items():\n",
    "#         key_name = key.replace(\"_\", \" \").title()\n",
    "#         if isinstance(value, dict):\n",
    "#             lines.append(f\"{prefix}{key_name}:\")\n",
    "#             lines.extend([f\"  {line}\" for line in json_to_text(value, prefix=prefix)])\n",
    "#         elif isinstance(value, list):\n",
    "#             if all(isinstance(i, dict) for i in value):\n",
    "#                 lines.append(f\"{prefix}{key_name}:\")\n",
    "#                 for item in value:\n",
    "#                     lines.extend([f\"  {line}\" for line in json_to_text(item, prefix=prefix)])\n",
    "#             else:\n",
    "#                 lines.append(f\"{prefix}{key_name}: {', '.join(map(str,value))}\")\n",
    "#         else:\n",
    "#             lines.append(f\"{prefix}{key_name}: {value}\")\n",
    "#     return lines\n",
    "\n",
    "# patient_record = \"Patient Past Record:\\n\" + \"\\n\".join(json_to_text(patient_record_json))\n",
    "\n",
    "# # -------------------------------\n",
    "# # 2. Retriever builder\n",
    "# # -------------------------------\n",
    "# def getRetriever():\n",
    "#     txt_documents = load_txts()\n",
    "#     chunked_docs = []\n",
    "\n",
    "#     for doc in txt_documents:\n",
    "#         # Escape curly braces in document content\n",
    "#         safe_doc_content = doc.page_content.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "#         # Prompt template\n",
    "#         prompt = ChatPromptTemplate.from_messages([\n",
    "#             (\"system\",\n",
    "#              \"You are a helpful assistant that splits medical or disease-related documents \"\n",
    "#              \"into semantically meaningful chunks for downstream semantic search and retrieval.\"),\n",
    "#             (\"human\",\n",
    "#              \"\"\"You will be given a document. Split it into semantically coherent sections.\n",
    "\n",
    "#              Guidelines:\n",
    "#              1. Do not omit any information.\n",
    "#              2. Keep related items together.\n",
    "#              3. Each chunk should be self-contained.\n",
    "#              4. Aim for ~800–1000 words per chunk.\n",
    "#              5. Include metadata (e.g., source) at the beginning of each chunk.\n",
    "#              6. Return output in strict JSON format as a list:\n",
    "#                 [{\"chunk\": \"text of chunk 1\", \"source\": \"source_name\"}, ...]\n",
    "\n",
    "#              Document:\n",
    "#              {document}\n",
    "#              \"\"\")\n",
    "#         ])\n",
    "\n",
    "#         # Correct formatting with escaped document\n",
    "#         formatted_prompt = prompt.format_messages(document=safe_doc_content)\n",
    "\n",
    "#         # Call your LLM\n",
    "#         response = llm.invoke(formatted_prompt)\n",
    "\n",
    "#         # Parse JSON safely\n",
    "#         try:\n",
    "#             chunks_json = json.loads(response.content)\n",
    "#             if not isinstance(chunks_json, list):\n",
    "#                 raise ValueError(\"LLM output is not a list\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: JSON parse failed for document {doc.metadata.get('source', 'unknown')}. Using fallback. Error: {e}\")\n",
    "#             chunks_json = [{\"chunk\": p.strip(), \"source\": doc.metadata.get(\"source\", \"unknown\")}\n",
    "#                            for p in doc.page_content.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "#         # Add chunks to list\n",
    "#         for idx, chunk_data in enumerate(chunks_json):\n",
    "#             if isinstance(chunk_data, dict):\n",
    "#                 page_content = chunk_data.get(\"chunk\", \"\")\n",
    "#                 source = chunk_data.get(\"source\", f\"{doc.metadata.get('source', 'unknown')}_chunk{idx+1}\")\n",
    "#             else:\n",
    "#                 page_content = str(chunk_data)\n",
    "#                 source = f\"{doc.metadata.get('source', 'unknown')}_chunk{idx+1}\"\n",
    "\n",
    "#             chunked_docs.append(\n",
    "#                 Document(\n",
    "#                     page_content=page_content,\n",
    "#                     metadata={\"source\": source, \"chunk_idx\": idx+1}\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     # Build vector store\n",
    "#     vector_store = Chroma.from_documents(\n",
    "#         chunked_docs,\n",
    "#         embedding_model,  # Ensure embedding_model is defined\n",
    "#         persist_directory=\"chroma_db\"\n",
    "#     )\n",
    "\n",
    "#     # Return retriever for top-k chunks\n",
    "#     retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages([\n",
    "#         (\"system\",\n",
    "#             \"You are a helpful medical assistant. Answer questions using only the provided context. \"\n",
    "#             \"If the answer is not in the context, say you don't know rather than making up information.\"\n",
    "#         ),\n",
    "#         (\"human\",\n",
    "#             \"\"\"You are given the patient's past medical records and the retrieved context from relevant documents.\n",
    "\n",
    "#             Context (retrieved from vector database):\n",
    "#             {context}\n",
    "\n",
    "#             Patient Past Record:\n",
    "#             {patient_record}\n",
    "\n",
    "#             Current patient query / symptom:\n",
    "#             {question}\n",
    "\n",
    "#             Guidelines:\n",
    "#             1. Use ONLY the information from the retrieved context and patient past record.\n",
    "#             2. Synthesize both sources to provide a clear, concise response.\n",
    "#             3. Do NOT invent facts not present in the context or past record.\n",
    "#             4. Provide your response in the following structured format:\n",
    "\n",
    "#                 - **Triage Level**: [Low / Medium / High / Emergency]\n",
    "#                 - **Reasoning**: [Explain why this triage level is assigned]\n",
    "#                 - **Urgent Evaluation Needed**: [Yes/No; specify tests if any]\n",
    "#                 - **Patient Actions**: [What patient should do next]\n",
    "#                 - **Clinician Tasks**: [Tasks clinician should perform]\n",
    "#                 - **Disclaimer**: [Include standard medical disclaimer]\n",
    "\n",
    "#             5. If the context and past record do not contain the answer, respond:\n",
    "#                 \"The provided documents do not contain information about this.\"\n",
    "#             \"\"\"\n",
    "#         )\n",
    "#     ])\n",
    "\n",
    "#     # Build RAG chain\n",
    "#     rag_chain = (\n",
    "#         {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#     )\n",
    "#     return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I'm very thirsty and peeing a lot, even at night.\"\n",
    "rag_chain = getRetriever()\n",
    "response = rag_chain.invoke(query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484dee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_util import find_matching_slots,book_slot,suggest_alternatives_json\n",
    "\n",
    "def doctor_appoint_agent(state: dict) -> dict:\n",
    "    print(\"Entered doctor_appoint_agent\")\n",
    "    preferred, alternatives = find_matching_slots(\"doctor\", state[\"doctor_name\"], state[\"preferred_start\"])\n",
    "    if preferred and book_slot(\"doctor\", state[\"doctor_name\"], state[\"preferred_start\"]):\n",
    "        state[\"status\"] = \"confirmed\"\n",
    "        state[\"booked_slot\"] = state[\"preferred_start\"]\n",
    "        state[\"output\"] = f\"Doctor appointment with {state['doctor_name']} confirmed for {state['preferred_start']}.\"\n",
    "    else:\n",
    "        state[\"status\"] = \"unavailable\"\n",
    "        state[\"alternatives\"] = suggest_alternatives_json(alternatives)\n",
    "        state[\"output\"] = f\"Requested slot unavailable. Alternatives provided.\"\n",
    "    return state\n",
    "\n",
    "from common_util import find_matching_slots,book_slot,suggest_alternatives_json\n",
    "\n",
    "def disease_appoint_agent(state: dict) -> dict:\n",
    "    preferred, alternatives = find_matching_slots(\"disease\", state[\"disease\"], state[\"preferred_start\"])\n",
    "    if preferred and book_slot(\"disease\", state[\"disease\"], state[\"preferred_start\"]):\n",
    "        state[\"status\"] = \"confirmed\"\n",
    "        state[\"booked_slot\"] = state[\"preferred_start\"]\n",
    "        state[\"output\"] = f\"Consultation for {state['disease']} with {state['specialty']} confirmed for {state['preferred_start']}.\"\n",
    "    else:\n",
    "        state[\"status\"] = \"unavailable\"\n",
    "        state[\"alternatives\"] = suggest_alternatives_json(alternatives)\n",
    "        state[\"output\"] = f\"Requested consultation slot unavailable. Alternatives provided.\"\n",
    "    return state\n",
    "\n",
    "def fallback_appoint_agent(state: dict) -> dict:\n",
    "    state[\"status\"] = \"fallback\"\n",
    "    state[\"output\"] = \"Sorry, I couldn’t classify your request into doctor, lab, disease, or service.\"\n",
    "    return state\n",
    "\n",
    "from common_util import find_matching_slots,book_slot,suggest_alternatives_json\n",
    "\n",
    "def lab_appoint_agent(state: dict) -> dict:\n",
    "    preferred, alternatives = find_matching_slots(\"lab\", state[\"test\"], state[\"preferred_start\"])\n",
    "    if preferred and book_slot(\"lab\", state[\"test\"], state[\"preferred_start\"]):\n",
    "        state[\"status\"] = \"confirmed\"\n",
    "        state[\"booked_slot\"] = state[\"preferred_start\"]\n",
    "        state[\"output\"] = f\"Lab test '{state['test']}' booked for {state['preferred_start']} at {state.get('location','default lab')}.\"\n",
    "    else:\n",
    "        state[\"status\"] = \"unavailable\"\n",
    "        state[\"alternatives\"] = suggest_alternatives_json(alternatives)\n",
    "        state[\"output\"] = f\"Requested lab slot unavailable. Alternatives provided.\"\n",
    "    return state\n",
    "\n",
    "from common_util import find_matching_slots,book_slot,suggest_alternatives_json\n",
    "\n",
    "def service_appoint_agent(state: dict) -> dict:\n",
    "    preferred, alternatives = find_matching_slots(\"service\", state[\"service\"], state[\"preferred_start\"])\n",
    "    if preferred and book_slot(\"service\", state[\"service\"], state[\"preferred_start\"]):\n",
    "        state[\"status\"] = \"confirmed\"\n",
    "        state[\"booked_slot\"] = state[\"preferred_start\"]\n",
    "        state[\"output\"] = f\"Service '{state['service']}' booked for {state['preferred_start']}.\"\n",
    "    else:\n",
    "        state[\"status\"] = \"unavailable\"\n",
    "        state[\"alternatives\"] = suggest_alternatives_json(alternatives)\n",
    "        state[\"output\"] = f\"Requested service slot unavailable. Alternatives provided.\"\n",
    "    return state\n",
    "\n",
    "# import sqlite3, json\n",
    "\n",
    "# def find_matching_slots(resource_type, resource_id, preferred_start):\n",
    "#     conn = sqlite3.connect(\"triage.db\")\n",
    "#     conn.row_factory = sqlite3.Row\n",
    "#     c = conn.cursor()\n",
    "\n",
    "#     # Check if preferred slot exists\n",
    "#     c.execute(\"\"\"\n",
    "#         SELECT * FROM availability\n",
    "#         WHERE resource_type=? AND resource_id=? AND slot_start=?\n",
    "#     \"\"\", (resource_type, resource_id, preferred_start))\n",
    "#     preferred = c.fetchone()\n",
    "\n",
    "#     # Get up to 5 alternative slots\n",
    "#     c.execute(\"\"\"\n",
    "#         SELECT * FROM availability\n",
    "#         WHERE resource_type=? AND resource_id=? AND is_available=1\n",
    "#         ORDER BY slot_start ASC\n",
    "#         LIMIT 5\n",
    "#     \"\"\", (resource_type, resource_id))\n",
    "#     alternatives = c.fetchall()\n",
    "\n",
    "#     conn.close()\n",
    "#     return preferred, alternatives\n",
    "\n",
    "# def suggest_alternatives_json(alts):\n",
    "#     return json.dumps([dict(r) for r in alts], indent=2)\n",
    "\n",
    "# def book_slot(resource_type, resource_id, slot_start):\n",
    "#     conn = sqlite3.connect(\"triage.db\")\n",
    "#     c = conn.cursor()\n",
    "#     c.execute(\"\"\"\n",
    "#         UPDATE availability\n",
    "#         SET is_available=0\n",
    "#         WHERE resource_type=? AND resource_id=? AND slot_start=? AND is_available=1\n",
    "#     \"\"\", (resource_type, resource_id, slot_start))\n",
    "#     ok = (c.rowcount == 1)\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "#     return ok\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "DB_FILE = \"triage.db\"\n",
    "\n",
    "def find_matching_slots(resource_type, resource_id, preferred_start):\n",
    "    \"\"\"Find the preferred slot and up to 5 alternative available slots.\"\"\"\n",
    "    with sqlite3.connect(DB_FILE) as conn:\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        c = conn.cursor()\n",
    "\n",
    "        # Preferred slot\n",
    "        c.execute(\n",
    "            \"\"\"\n",
    "            SELECT * FROM availability\n",
    "            WHERE resource_type=? AND resource_id=? AND slot_start=?\n",
    "            \"\"\",\n",
    "            (resource_type, resource_id, preferred_start),\n",
    "        )\n",
    "        preferred = c.fetchone()\n",
    "\n",
    "        # Alternatives\n",
    "        c.execute(\n",
    "            \"\"\"\n",
    "            SELECT * FROM availability\n",
    "            WHERE resource_type=? AND resource_id=? AND is_available=1\n",
    "            ORDER BY slot_start ASC\n",
    "            LIMIT 5\n",
    "            \"\"\",\n",
    "            (resource_type, resource_id),\n",
    "        )\n",
    "        alternatives = c.fetchall()\n",
    "\n",
    "    return preferred, alternatives\n",
    "\n",
    "\n",
    "def suggest_alternatives_json(alts):\n",
    "    \"\"\"Convert alternative slots into JSON format.\"\"\"\n",
    "    return json.dumps([dict(r) for r in alts], indent=2)\n",
    "\n",
    "\n",
    "def book_slot(resource_type, resource_id, slot_start):\n",
    "    \"\"\"Book a slot by marking it unavailable. Returns True if successful.\"\"\"\n",
    "    with sqlite3.connect(DB_FILE) as conn:\n",
    "        c = conn.cursor()\n",
    "        c.execute(\n",
    "            \"\"\"\n",
    "            UPDATE availability\n",
    "            SET is_available=0\n",
    "            WHERE resource_type=? AND resource_id=? AND slot_start=? AND is_available=1\n",
    "            \"\"\",\n",
    "            (resource_type, resource_id, slot_start),\n",
    "        )\n",
    "        ok = (c.rowcount == 1)\n",
    "        conn.commit()\n",
    "\n",
    "    return ok\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e53baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36b22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
