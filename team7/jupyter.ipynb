{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3519b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(2+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd4aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GenaiblrpioUsr2\\Desktop\\Team24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8297801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found ✅\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# file = r\"C:\\Users\\GenaiblrpioUsr2\\Desktop\\Team24\\database\\data\\inventory.xlsx\"\n",
    "file = r\"C:\\Users\\GenaiblrpioUsr2\\Desktop\\Team24\\database\\data\\pdfs\"\n",
    "\n",
    "if os.path.exists(file):\n",
    "    print(\"File found ✅\")\n",
    "else:\n",
    "    print(\"File not found ❌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6a202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU101: {'name': 'Laptop', 'stock': 25, 'threshold': 10, 'category': 'Electronics', 'unit_price': 800, 'supplier': 'TechWorld', 'warehouse': 'Warehouse A'}\n",
      "SKU102: {'name': 'Chair', 'stock': 120, 'threshold': 50, 'category': 'Furniture', 'unit_price': 45, 'supplier': 'FurniCo', 'warehouse': 'Warehouse B'}\n",
      "SKU103: {'name': 'Printer Ink', 'stock': 8, 'threshold': 15, 'category': 'Office Supply', 'unit_price': 20, 'supplier': 'InkHouse', 'warehouse': 'Warehouse A'}\n",
      "SKU104: {'name': 'Desk Lamp', 'stock': 60, 'threshold': 30, 'category': 'Furniture', 'unit_price': 25, 'supplier': 'BrightCo', 'warehouse': 'Warehouse C'}\n",
      "SKU105: {'name': 'USB Cable', 'stock': 12, 'threshold': 20, 'category': 'Electronics', 'unit_price': 5, 'supplier': 'CableMart', 'warehouse': 'Warehouse A'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_inventory():\n",
    "    file = r\"C:\\Users\\GenaiblrpioUsr2\\Desktop\\Team24\\database\\data\\inventory.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # normalize column names to lowercase\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    \n",
    "    inv = {} \n",
    "    for _, r in df.iterrows():\n",
    "        inv[r[\"item_id\"].upper()] = {\n",
    "            \"name\": r[\"product_name\"],\n",
    "            \"stock\": int(r[\"quantity\"]),\n",
    "            \"threshold\": int(r[\"threshold\"]),\n",
    "            \"category\": r[\"category\"],\n",
    "            \"unit_price\": r[\"unit_price\"],\n",
    "            \"supplier\": r[\"supplier\"],\n",
    "            \"warehouse\" : r[\"warehouse\"]\n",
    "        }\n",
    "    return inv\n",
    "\n",
    "# Test the function\n",
    "inventory = load_inventory()\n",
    "for sku, details in inventory.items():\n",
    "    print(f\"{sku}: {details}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a64eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAPTOP: {'name': 'Electronics', 'stock': 10, 'threshold': 800, 'category': 25, 'unit_price': 'TechWorld', 'supplier': 'Warehouse A'}\n",
      "CHAIR: {'name': 'Furniture', 'stock': 50, 'threshold': 45, 'category': 120, 'unit_price': 'FurniCo', 'supplier': 'Warehouse B'}\n",
      "PRINTER INK: {'name': 'Office Supply', 'stock': 15, 'threshold': 20, 'category': 8, 'unit_price': 'InkHouse', 'supplier': 'Warehouse A'}\n",
      "DESK LAMP: {'name': 'Furniture', 'stock': 30, 'threshold': 25, 'category': 60, 'unit_price': 'BrightCo', 'supplier': 'Warehouse C'}\n",
      "USB CABLE: {'name': 'Electronics', 'stock': 20, 'threshold': 5, 'category': 12, 'unit_price': 'CableMart', 'supplier': 'Warehouse A'}\n"
     ]
    }
   ],
   "source": [
    "inventory = load_inventory()   # ✅ Call the function\n",
    "for k, v in inventory.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a457cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import httpx\n",
    "\n",
    "# Create an HTTP client that skips SSL verification (only for hackathon/test environments)\n",
    "client = httpx.Client(verify=False)\n",
    "llm = ChatOpenAI(\n",
    " base_url=\"https://genailab.tcs.in\",\n",
    " model=\"azure/genailab-maas-gpt-4o\",\n",
    " api_key=\"sk-u6zTQaiDKlhHn4-k_hhihw\",\n",
    " http_client=client\n",
    ")\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(\n",
    " base_url=\"https://genailab.tcs.in\",\n",
    " model=\"azure/genailab-maas-text-embedding-3-large\",\n",
    " api_key=\"sk-u6zTQaiDKlhHn4-k_hhihw\",\n",
    " http_client=client)\n",
    "\n",
    "import requests\n",
    "for method in (\"get\",\"post\",\"put\",\"delete\",\"head\",\"options\",\"patch\"):\n",
    "    original = getattr(requests,method)\n",
    "\n",
    "    def insecure_request(*args, _original = original, **kwargs):\n",
    "        kwargs[\"verify\"] = False\n",
    "        return _original(*args,**kwargs)\n",
    "    \n",
    "    setattr(requests,method,insecure_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fac07d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from database.loader import load_pdfs\n",
    "\n",
    "pdf_folder =  r\"C:\\Users\\GenaiblrpioUsr2\\Desktop\\Team24\\database\\data\\pdfs\"\n",
    "pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "# Load each PDF and store chunks\n",
    "all_docs1 = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    docs = loader.load()  # List[Document]\n",
    "    \n",
    "    # Optionally, add source info to each document\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"source\"] = os.path.basename(pdf_file)\n",
    "        all_docs1.append(doc)\n",
    "        \n",
    "full_text1 = \"\\n\".join([doc.page_content for doc in all_docs1])\n",
    "\n",
    "txt_folder = r\"C:\\Users\\GenaiblrpioUsr2\\Desktop\\Team24\\database\\data\\txts\"\n",
    "txt_files = [os.path.join(txt_folder, f) for f in os.listdir(txt_folder) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "            # Create a simple document-like dict (similar to PDF loader output)\n",
    "        doc = {\n",
    "            \"page_content\": content,\n",
    "            \"metadata\": {\"source\": os.path.basename(txt_file)}\n",
    "        }\n",
    "    all_docs.append(doc)\n",
    "\n",
    " # Combine all text into one big string\n",
    "full_text = \"\\n\".join([doc[\"page_content\"] for doc in all_docs])\n",
    "\n",
    "combined = full_text + full_text1\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that chunks documents for semantic search.\"),\n",
    "    (\"human\", \"\"\"Split the following document into semantically meaningful sections. \n",
    "Follow these rules:\n",
    "1. Do not omit any information — every detail must appear in some chunk.\n",
    "2. Keep related items (like lists of dates, events, or tables) together in the same chunk.\n",
    "3. Each chunk should be self-contained and understandable without needing other chunks.\n",
    "4. Limit chunk size to about 5000–6000 words. If longer or Lesser, split carefully without breaking sentences or lists.\n",
    "5. Return the chunks as a numbered list, with each chunk clearly separated.\n",
    "\n",
    "Document:\n",
    "{document}\"\"\")\n",
    "])\n",
    "\n",
    "formatted_prompt = prompt.format_messages(document=combined)\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "chunks = response.content.split(\"\\n\\n\")  # crude split; refine if needed\n",
    "\n",
    "chunked_docs = [Document(page_content=chunk.strip()) for chunk in chunks if chunk.strip()]\n",
    "\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunked_docs,\n",
    "    embedding_model,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "vector_store.persist()\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on retrieved documents.\"),\n",
    "    (\"human\", \"\"\"Using the following context, provide a complete answer. \n",
    "    Do not omit any relevant details. \n",
    "    If multiple chunks contain related information, merge them into a single coherent answer. Don't miss any kind of data.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\"\"\")\n",
    "])\n",
    "\n",
    "# Chain: retrieve → format prompt → invoke LLM\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93aff804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content indicates that the document has been organized into semantically meaningful sections to provide a clear and logical breakdown. These sections are designed to convey the main themes and insights from the document effectively. This structured approach ensures that the key ideas and information are presented in a coherent and digestible way for the reader.\n"
     ]
    }
   ],
   "source": [
    "query = \"Summary of the content\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2fc5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from database.loader import load_pdfs\n",
    "\n",
    "pdf_folder =  r\"C:\\Users\\GenaiblrpioUsr2\\Desktop\\Team24\\database\\data\\pdfs\"\n",
    "pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "# Load each PDF and store chunks\n",
    "all_docs = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    docs = loader.load()  # List[Document]\n",
    "    \n",
    "    # Optionally, add source info to each document\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"source\"] = os.path.basename(pdf_file)\n",
    "        all_docs.append(doc)\n",
    "        \n",
    "full_text = \"\\n\".join([doc.page_content for doc in all_docs])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that chunks documents for semantic search.\"),\n",
    "    (\"human\", \"Split the following document into semantically meaningful sections. Return each chunk as a numbered list.\\n\\n{document}\")\n",
    "])\n",
    "\n",
    "formatted_prompt = prompt.format_messages(document=full_text)\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "chunks = response.content.split(\"\\n\\n\")  # crude split; refine if needed\n",
    "\n",
    "chunked_docs = [Document(page_content=chunk.strip()) for chunk in chunks if chunk.strip()]\n",
    "\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunked_docs,\n",
    "    embedding_model,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "vector_store.persist()\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a market insights retrieval agent.\"\n",
    "    \"Retrieve relevant information to forecast 12-month sales for Laptop.\"   \n",
    "    \"Focus on:\"\n",
    "      \"1. FMCG sector growth (volume/value growth, GST impacts)\"\n",
    "      \"2. Rural vs urban market performance and trends\"\n",
    "      \"3. Category-specific insights (staples, HPC, food, etc.)\"\n",
    "      \"4. Longitudinal trends over quarters\"\n",
    "      \"5. Factors affecting growth (promotions, smaller packs, macro disruptions)\"\n",
    "    \"Return only the most relevant chunks with:\"\n",
    "     \"- source_id\"\n",
    "      \"- chunk_text\"\n",
    "      \"- chunk_date if available\"\n",
    "      \"- relevance score\"),\n",
    "    (\"human\", \"Answer the question using the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Chain: retrieve → format prompt → invoke LLM\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "102c5374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the most relevant insights:\n",
      "\n",
      "1. **Rural vs Urban Market Performance and Trends**:\n",
      "   - Rural markets have consistently outpaced urban markets, with higher growth rates observed in previous quarters (e.g., March and September Quarters 2025 with 8.3% and 8.4% growth, respectively). This holds potential for continued demand in rural markets for laptops, particularly if they are positioned as essential productivity tools.\n",
      "\n",
      "2. **Impact of GST Changes**:\n",
      "   - GST changes are noted to have slowed FMCG volume growth in the September quarter. For laptops, this could imply a temporary disruption in consumer purchasing behavior until the adjustments stabilize over the next two quarters. However, this impact may ease as inflation shows signs of improving.\n",
      "\n",
      "3. **Category-Specific and Longitudinal Trends**:\n",
      "   - While demand for staples remains steady, impulse categories and habit-forming products are seeing a decline, indicating a shift in consumer prioritization toward essentials. Over-the-counter (OTC) categories benefited from price-driven value growth, showcasing that premium or high-value items saw sustained demand. This trend might translate to consumer electronics, where premium laptops might gain traction over basics, especially in urban e-commerce markets.\n",
      "\n",
      "4. **Factors Affecting Growth**:\n",
      "   - Declining offline sales in metropolitan areas hint at a continuing rise in e-commerce, which could favor laptop sales through online platforms. Promotional strategies, such as discounts and partnerships, paired with the potential for inflation to ease, could stimulate demand.\n",
      "\n",
      "**Forecast Summary**: \n",
      "Laptop sales may experience varied performance across rural and urban markets in the next 12 months. Rural markets, driven by steady growth trends, show strong potential. Urban markets may rely on e-commerce and value-premium segments for recovery. GST changes could impact the initial quarters but are likely to stabilize, aligning with the easing inflation trend.\n"
     ]
    }
   ],
   "source": [
    "query = \"Forecast the next 12 months of sales for Laptops\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
