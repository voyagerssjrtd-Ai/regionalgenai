{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8010d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import httpx\n",
    "\n",
    "# Create an HTTP client that skips SSL verification (only for hackathon/test environments)\n",
    "client = httpx.Client(verify=False)\n",
    "llm = ChatOpenAI(\n",
    " base_url=\"https://genailab.tcs.in\",\n",
    " model=\"azure/genailab-maas-gpt-4o\",\n",
    " api_key=\"sk-K5cCB7yk7rc3ex2NV3-TWw\",\n",
    " http_client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a62905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(\n",
    " base_url=\"https://genailab.tcs.in\",\n",
    " model=\"azure/genailab-maas-text-embedding-3-large\",\n",
    " api_key=\"sk-K5cCB7yk7rc3ex2NV3-TWw\",\n",
    " http_client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85977fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "for method in (\"get\",\"post\",\"put\",\"delete\",\"head\",\"options\",\"patch\"):\n",
    "    original = getattr(requests,method)\n",
    "\n",
    "    def insecure_request(*args, _original = original, **kwargs):\n",
    "        kwargs[\"verify\"] = False\n",
    "        return _original(*args,**kwargs)\n",
    "    \n",
    "    setattr(requests,method,insecure_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c52f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\genaiblrancusr103\\Desktop\\TEST\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23623d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518fb163",
   "metadata": {},
   "source": [
    "WIHTOUT LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# # --- Load CSV ---\n",
    "# csv_path = r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\csvfiles\\Dairy_Historical_Sales.csv\"\n",
    "# df = pd.read_csv(csv_path)\n",
    "# df[\"Month\"] = df[\"Month\"].astype(str)\n",
    "\n",
    "# # --- Create row-level Documents ---\n",
    "# row_docs = []\n",
    "# for _, row in df.iterrows():\n",
    "#     page = (\n",
    "#         f\"Month: {row['Month']}, Milk: {row['Milk']}, Ghee: {row['Ghee']}, \"\n",
    "#         f\"Curd: {row['Curd']}, Paneer: {row['Paneer']}, Cheese: {row['Cheese']}, \"\n",
    "#         f\"Sweets: {row['Sweets']}\"\n",
    "#     )\n",
    "#     row_docs.append(\n",
    "#         Document(\n",
    "#             page_content=page,\n",
    "#             metadata={\n",
    "#                 \"month\": row[\"Month\"],\n",
    "#                 \"year\": row[\"Month\"][:4],\n",
    "#                 \"Milk\": int(row[\"Milk\"]),\n",
    "#                 \"Ghee\": int(row[\"Ghee\"]),\n",
    "#                 \"Curd\": int(row[\"Curd\"]),\n",
    "#                 \"Paneer\": int(row[\"Paneer\"]),\n",
    "#                 \"Cheese\": int(row[\"Cheese\"]),\n",
    "#                 \"Sweets\": int(row[\"Sweets\"]),\n",
    "#             },\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# # --- Create yearly chunks ---\n",
    "# chunk_docs = []\n",
    "# for year, group in df.groupby(df[\"Month\"].str[:4]):\n",
    "#     content_lines = [f\"Year: {year}\", \"Month Milk Ghee Curd Paneer Cheese Sweets\"]\n",
    "#     for _, r in group.iterrows():\n",
    "#         content_lines.append(\n",
    "#             f\"{r['Month']} {r['Milk']} {r['Ghee']} {r['Curd']} {r['Paneer']} {r['Cheese']} {r['Sweets']}\"\n",
    "#         )\n",
    "#     chunk_docs.append(\n",
    "#         Document(page_content=\"\\n\".join(content_lines), metadata={\"year\": year, \"type\": \"year_chunk\"})\n",
    "#     )\n",
    "\n",
    "# # --- Merge row-level + yearly chunks ---\n",
    "# all_docs = row_docs + chunk_docs\n",
    "\n",
    "# # --- Store in Chroma vector DB ---\n",
    "# persist_dir = \"chroma_db_dairy\"\n",
    "# vector_store = Chroma.from_documents(documents=all_docs, embedding=embedding_model, persist_directory=persist_dir)\n",
    "# vector_store.persist()\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "# # --- Smart query handler ---\n",
    "# def smart_query(query: str):\n",
    "#     print(f\"\\n--- Query: {query} ---\")\n",
    "\n",
    "#     months_map = {\n",
    "#         \"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\", \"May\": \"05\", \"Jun\": \"06\",\n",
    "#         \"Jul\": \"07\", \"Aug\": \"08\", \"Sep\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"\n",
    "#     }\n",
    "\n",
    "#     tokens = query.split()\n",
    "#     year = None\n",
    "#     month_num = None\n",
    "#     product = None\n",
    "\n",
    "#     # crude parse: look for year, month, product\n",
    "#     for t in tokens:\n",
    "#         if t.isdigit() and len(t) == 4:\n",
    "#             year = t\n",
    "#         if t[:3].capitalize() in months_map:\n",
    "#             month_num = months_map[t[:3].capitalize()]\n",
    "#         if t.capitalize() in [\"Milk\", \"Ghee\", \"Curd\", \"Paneer\", \"Cheese\", \"Sweets\"]:\n",
    "#             product = t.capitalize()\n",
    "\n",
    "#     # Case 1: month + product + year → exact lookup\n",
    "#     if year and month_num and product:\n",
    "#         target_month = f\"{year}-{month_num}\"\n",
    "#         exact = [doc for doc in row_docs if doc.metadata[\"month\"] == target_month]\n",
    "#         if exact:\n",
    "#             print(f\"{product} sales in {target_month}: {exact[0].metadata[product]}\")\n",
    "#             return\n",
    "\n",
    "#     # Case 2: product + year (no month) → aggregate\n",
    "#     if year and product and not month_num:\n",
    "#         year_docs = [doc for doc in row_docs if doc.metadata[\"year\"] == year]\n",
    "#         if year_docs:\n",
    "#             total = sum(doc.metadata[product] for doc in year_docs)\n",
    "#             breakdown = \"\\n\".join([f\"{d.metadata['month']} → {d.metadata[product]}\" for d in year_docs])\n",
    "#             print(f\"{product} sales in {year}:\\n{breakdown}\\nTotal {product} sales in {year}: {total}\")\n",
    "#             return\n",
    "\n",
    "#     # Case 3: fallback → use retriever + LLM summarization\n",
    "#     results = retriever.invoke(query)\n",
    "#     context = \"\\n\".join([d.page_content for d in results])\n",
    "#     prompt = f\"Answer the question based on the data:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "#     answer = llm.invoke(prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a3132",
   "metadata": {},
   "source": [
    "WITH LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa0062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "49217edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant that answers questions based on retrieved documents.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Answer the question using the following context:\\n\\n[Document(metadata={'type': 'year_chunk', 'year': '2025'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106'), Document(metadata={'year': '2025', 'type': 'year_chunk'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106'), Document(metadata={'type': 'year_chunk', 'year': '2025'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106'), Document(metadata={'type': 'year_chunk', 'year': '2025'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106'), Document(metadata={'type': 'year_chunk', 'year': '2025'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106'), Document(metadata={'type': 'year_chunk', 'year': '2025'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106'), Document(metadata={'year': '2025', 'type': 'year_chunk'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106'), Document(metadata={'type': 'year_chunk', 'year': '2025'}, page_content='Year: 2025\\\\nMonth Milk Ghee Curd Paneer Cheese Sweets\\\\n2025-01 717 143 215 71 57 107\\\\n2025-02 696 139 209 69 56 104\\\\n2025-03 775 155 233 77 62 116\\\\n2025-04 796 159 239 79 64 119\\\\n2025-05 858 171 257 85 69 129\\\\n2025-06 826 166 248 82 66 124\\\\n2025-07 783 157 235 78 63 118\\\\n2025-08 740 148 221 74 59 111\\\\n2025-09 702 140 210 70 56 105\\\\n2025-10 706 141 212 71 57 106')]\\n\\nQuestion: Milk sales in Jan 2025\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# --- Example Queries ---\n",
    "queries = [\n",
    "    \"Milk sales in Jan 2025\",\n",
    "    \"Ghee sales in Feb 2014\",\n",
    "    \"Ghee sales in 2025\",\n",
    "    \"Summarize Cheese sales in 2025\",\n",
    "    \"All sales in 2025\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n--- Query: {q} ---\")\n",
    "    result = rag_chain.invoke({\"input\": q})\n",
    "    print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b30d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    all_docs,\n",
    "    embedding_model,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9ef59828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on retrieved documents.\"),\n",
    "    (\"human\", \"Answer the question using the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6baac988",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Number of requested results 0, cannot be negative, or zero. in query.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# query = \"Given a prediction for Ghee for 2026 year. Whether I need to make more orders for upcoming month?\"\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mGive of Cheese sales for entire 2024\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3127\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3127\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3128\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3129\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3854\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3848\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3849\u001b[39m         futures = [\n\u001b[32m   3850\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3851\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3852\u001b[39m         ]\n\u001b[32m   3853\u001b[39m         output = {\n\u001b[32m-> \u001b[39m\u001b[32m3854\u001b[39m             key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3855\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3856\u001b[39m         }\n\u001b[32m   3857\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3837\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3831\u001b[39m child_config = patch_config(\n\u001b[32m   3832\u001b[39m     config,\n\u001b[32m   3833\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3834\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3835\u001b[39m )\n\u001b[32m   3836\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py:216\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    220\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1032\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1030\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1034\u001b[39m     docs_and_similarities = (\n\u001b[32m   1035\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1036\u001b[39m             query, **kwargs_\n\u001b[32m   1037\u001b[39m         )\n\u001b[32m   1038\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:350\u001b[39m, in \u001b[36mChroma.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, **kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    334\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    335\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    338\u001b[39m     **kwargs: Any,\n\u001b[32m    339\u001b[39m ) -> List[Document]:\n\u001b[32m    340\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[32m    341\u001b[39m \n\u001b[32m    342\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m \u001b[33;03m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:440\u001b[39m, in \u001b[36mChroma.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, where_document, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    439\u001b[39m     query_embedding = \u001b[38;5;28mself\u001b[39m._embedding_function.embed_query(query)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__query_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_core\\utils\\utils.py:52\u001b[39m, in \u001b[36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     msg = (\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExactly one argument in each of the following\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m groups must be defined:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:157\u001b[39m, in \u001b[36mChroma.__query_collection\u001b[39m\u001b[34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    154\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import chromadb python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install chromadb`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:215\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    172\u001b[39m     query_embeddings: Optional[\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m     ],\n\u001b[32m    190\u001b[39m ) -> QueryResult:\n\u001b[32m    191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    212\u001b[39m \n\u001b[32m    213\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     query_request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_prepare_query_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_uris\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_uris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     query_results = \u001b[38;5;28mself\u001b[39m._client._query(\n\u001b[32m    228\u001b[39m         collection_id=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    229\u001b[39m         ids=query_request[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    240\u001b[39m         response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    241\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:103\u001b[39m, in \u001b[36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> T:\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    105\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:331\u001b[39m, in \u001b[36mCollectionCommon._validate_and_prepare_query_request\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    329\u001b[39m validate_filter_set(filter_set=filters)\n\u001b[32m    330\u001b[39m validate_include(include=include)\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[43mvalidate_n_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Prepare\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query_records[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\genaiblrancusr103\\Desktop\\TEST\\venv\\Lib\\site-packages\\chromadb\\api\\types.py:1259\u001b[39m, in \u001b[36mvalidate_n_results\u001b[39m\u001b[34m(n_results)\u001b[39m\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1256\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected requested number of results to be a int, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1257\u001b[39m     )\n\u001b[32m   1258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_results <= \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1260\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of requested results \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, cannot be negative, or zero.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1261\u001b[39m     )\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_results\n",
      "\u001b[31mTypeError\u001b[39m: Number of requested results 0, cannot be negative, or zero. in query."
     ]
    }
   ],
   "source": [
    "# # query = \"Given a prediction for Ghee for 2026 year. Whether I need to make more orders for upcoming month?\"\n",
    "# query = \"Give of Cheese sales for entire 2024\"\n",
    "# response = rag_chain.invoke(query)\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain_community.document_loaders import PyPDFLoader, CSVLoader\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# # --- Load multiple PDFs ---\n",
    "# pdf_files = [\n",
    "#     r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\pdffiles\\Food_Beverages_Events_2025_2026.pdf\",\n",
    "#     r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\pdffiles\\India_FMCG_outlook_2025.pdf\",\n",
    "#     r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\pdffiles\\Market_news_on_FMCG.pdf\",\n",
    "#     r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\pdffiles\\Retail_marketing_calendar_2026.pdf\",\n",
    "#     r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\pdffiles\\Snipp_2025_Retail_Seasonal_Promotion_Marketing_Calendar.pdf\"\n",
    "# ]\n",
    "\n",
    "# pdf_docs = []\n",
    "# for file in pdf_files:\n",
    "#     loader = PyPDFLoader(file)\n",
    "#     pdf_docs.extend(loader.load())\n",
    "\n",
    "# # --- Load CSV file (optional, if you want to combine) ---\n",
    "# csv_loader = CSVLoader(file_path=r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\csvfiles\\Dairy_Historical_Sales.csv\")\n",
    "# csv_docs = csv_loader.load()\n",
    "\n",
    "# # --- Combine all sources ---\n",
    "# all_docs = pdf_docs + csv_docs\n",
    "\n",
    "# # Combine all text into one block\n",
    "# full_text = \"\\n\".join([doc.page_content for doc in all_docs])\n",
    "\n",
    "# # Agentic chunking prompt\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\",\n",
    "#      \"You are a helpful assistant that chunks documents into semantically meaningful sections for retrieval. \"\n",
    "#      \"Apply different strategies depending on the document type:\\n\"\n",
    "#      \" - For CSV/tabular data: group related rows together by logical dimensions such as time periods (e.g., yearly or quarterly), \"\n",
    "#      \"product categories, or other natural groupings. Preserve the tabular structure in each chunk.\\n\"\n",
    "#      \" - For PDF/textual documents: split content into coherent sections based on headings, subheadings, or semantic meaning. \"\n",
    "#      \"Keep paragraphs or related sentences together so that each chunk represents a complete idea.\\n\"\n",
    "#      \"Ensure chunks are concise but contextually complete.\"),\n",
    "#     (\"human\",\n",
    "#      \"Split the following content into semantically meaningful chunks according to its type (CSV vs PDF). \"\n",
    "#      \"For CSVs, group rows logically (e.g., by year, product, or category). \"\n",
    "#      \"For PDFs, group paragraphs or sections by semantic meaning. \"\n",
    "#      \"Return each chunk as a numbered list.\\n\\n{document}\")\n",
    "# ])\n",
    "\n",
    "# # Format prompt with combined content\n",
    "# formatted_prompt = prompt.format_messages(document=full_text)\n",
    "\n",
    "# # Invoke LLM for agentic chunking\n",
    "# response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# # Parse response into chunks\n",
    "# chunks = response.content.split(\"\\n\\n\")  # crude split; refine if needed\n",
    "\n",
    "# # Convert chunks into LangChain Document objects\n",
    "# chunked_docs = [Document(page_content=chunk.strip()) for chunk in chunks if chunk.strip()]\n",
    "\n",
    "\n",
    "\n",
    "# # Agentic chunking prompt\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant that chunks tabular data into semantically meaningful sections for retrieval.\"),\n",
    "#     (\"human\", \"Split the following CSV content into semantically meaningful chunks. \"\n",
    "#               \"Group related rows together (e.g., by time period, category, or logical grouping). \"\n",
    "#               \"Return each chunk as a numbered list.\\n\\n{document}\")\n",
    "# ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain_community.document_loaders import CSVLoader\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# # Use CSVLoader instead of PyPDFLoader\n",
    "# loader = CSVLoader(file_path=r\"c:\\Users\\genaiblrancusr103\\Desktop\\TEST\\csvfiles\\Dairy_Historical_Sales.csv\")\n",
    "# docs = loader.load()\n",
    "\n",
    "# # Combine all rows into one text block\n",
    "# full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\",\n",
    "#      \"You are a helpful assistant that chunks documents into semantically meaningful sections for retrieval. \"\n",
    "#      \"Apply different strategies depending on the document type:\\n\"\n",
    "#      \" - For CSV/tabular data: group related rows together by logical dimensions such as time periods (e.g., yearly or quarterly), \"\n",
    "#      \"product categories, or other natural groupings. Preserve the tabular structure in each chunk.\\n\"\n",
    "#      \" - For PDF/textual documents: split content into coherent sections based on headings, subheadings, or semantic meaning. \"\n",
    "#      \"Keep paragraphs or related sentences together so that each chunk represents a complete idea.\\n\"\n",
    "#      \"Ensure chunks are concise but contextually complete.\"),\n",
    "#     (\"human\",\n",
    "#      \"Split the following content into semantically meaningful chunks according to its type (CSV vs PDF). \"\n",
    "#      \"For CSVs, group rows logically (e.g., by year, product, or category). \"\n",
    "#      \"For PDFs, group paragraphs or sections by semantic meaning. \"\n",
    "#      \"Return each chunk as a numbered list.\\n\\n{document}\")\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Format prompt with document content\n",
    "# formatted_prompt = prompt.format_messages(document=full_text)\n",
    "\n",
    "# # Invoke LLM for agentic chunking\n",
    "# response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# # Parse response into chunks\n",
    "# chunks = response.content.split(\"\\n\\n\")  # crude split; refine if needed\n",
    "\n",
    "# # Convert chunks into LangChain Document objects\n",
    "# chunked_docs = [Document(page_content=chunk.strip()) for chunk in chunks if chunk.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# vector_store = Chroma.from_documents(\n",
    "#     chunked_docs,\n",
    "#     embedding_model,\n",
    "#     persist_directory=\"chroma_db\"\n",
    "# )\n",
    "# vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.runnables import RunnableMap\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\",\n",
    "#      \"You are a Sales Analyzer and forecasting assistant that predicts future monthly sales for dairy products.\\n\"\n",
    "#      \"You need to act as assistant according to the Query which is coming from the user. \\n\"\n",
    "#      \"Ground your work in:\\n\"\n",
    "#      \"1) Dairy historical sales CSV (columns: Month, Milk, Ghee, Curd, Paneer, Cheese, Sweets).\\n\"\n",
    "#      \"2) External market documents: Food_Beverages_Events_2025_2026.pdf, India_FMCG_outlook_2025.pdf, \"\n",
    "#      \"Market_news_on_FMCG.pdf, Retail_marketing_calendar_2026.pdf, \"\n",
    "#      \"Snipp_2025_Retail_Seasonal_Promotion_Marketing_Calendar.pdf.\\n\\n\"\n",
    "#      \"Strict rules:\\n\"\n",
    "#      \"- Use the entire historical dataset for forecasting.\\n\"\n",
    "#      \"- If the question mentions one product, return only that product; if multiple, return product-wise.\\n\"\n",
    "#      \"- The output table must first list the last 10 rows of historical sales for each requested product, \"\n",
    "#      \"with the Sales column containing the exact numeric values copied verbatim from the provided context. \"\n",
    "#      \"Never replace numeric values with text like 'Historical Data'.\\n\"\n",
    "#      \"- After the 10 historical rows, append predicted rows for the requested months.\\n\"\n",
    "#      \"- Predictions must consider seasonal patterns, event calendars, promotions, and market news as Key Drivers.\\n\"\n",
    "#      \"- Highlight Risks/Uncertainty when present (e.g., supply chain issues, policy changes, macro slowdown).\\n\"\n",
    "#      \"- If the question specifies months/years, predict exactly those; otherwise predict the next 6 months beyond \"\n",
    "#      \"the latest historical month when the question asks for 'next 6 months'.\\n\"\n",
    "#      \"- Date formats: Month = YYYY-MM; Year = 4-digit year taken from the Month.\\n\"\n",
    "#      \"- If a historical value is not present in the context for a given Month/Product, write 'Insufficient context' \"\n",
    "#      \"in Sales for that row and explain in Key Drivers; do not invent numbers.\\n\"\n",
    "#      \"- Output must be a single table with these columns only:\\n\"\n",
    "#      \"  Month | Year | Product | Sales | Key Drivers | Risks/Uncertainty.\\n\"),\n",
    "#     (\"human\",\n",
    "#      \"Using the following combined context (historical CSV + external market documents):\\n\\n{context}\\n\\n\"\n",
    "#      \"Task:\\n\"\n",
    "#      \"1) Identify the products and months mentioned in the question.\\n\"\n",
    "#      \"2) Output the last 10 rows of historical sales for each requested product, copying the Sales values exactly from the context.\\n\"\n",
    "#      \"3) Append predicted rows for the requested months (e.g., 'next 6 months'), with numeric Sales values.\\n\"\n",
    "#      \"4) Provide concise Key Drivers (seasonality, events, promotions, market news) and note Risks/Uncertainty when applicable.\\n\\n\"\n",
    "#      \"Return only the table with the exact columns specified, no prose.\\n\\n\"\n",
    "#      \"Question: {question}\")\n",
    "# ])\n",
    "\n",
    "# # Chain: retrieve → format prompt → invoke LLM\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.runnables import RunnableMap\n",
    "\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "# # prompt = ChatPromptTemplate.from_messages([\n",
    "# #     (\"system\", \n",
    "# #      \"You are a helpful assistant that predicts future sales based on historical sales data. \"\n",
    "# #      \"Use the retrieved context (past monthly sales) to identify trends, seasonality, and patterns. \"\n",
    "# #      \"Always return the output in a clear tabular format with columns for Month, Year, and Predicted Sales.\"),\n",
    "# #     (\"human\", \n",
    "# #      \"Using the following historical sales context:\\n\\n{context}\\n\\n\"\n",
    "# #      \"Predict the sales for the months mentioned in the question. \"\n",
    "# #      \"Return the results strictly in tabular format.\\n\\nQuestion: {question}\")\n",
    "# # ])\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant that answers questions based on retrieved documents.\"),\n",
    "#     (\"human\", \"Answer the question using the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Chain: retrieve → format prompt → invoke LLM\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe328f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided documents do not mention ghee sales for July 2025 specifically. The text emphasizes trends and patterns but lacks explicit monthly numerical data for ghee or other products for that specific time. For precise figures, direct access to the detailed monthly sales data would be necessary.\n"
     ]
    }
   ],
   "source": [
    "# # query = \"Given a prediction for Ghee for 2026 year. Whether I need to make more orders for upcoming month?\"\n",
    "# query = \"what are Ghee sales in July 2025\"\n",
    "# response = rag_chain.invoke(query)\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# import httpx\n",
    "\n",
    "# # Create an HTTP client that skips SSL verification (only for hackathon/test environments)\n",
    "# client = httpx.Client(verify=False)\n",
    "# llm = ChatOpenAI(\n",
    "#  base_url=\"https://genailab.tcs.in\",\n",
    "#  model=\"azure/genailab-maas-gpt-4o\",\n",
    "#  api_key=\"sk-A5iYc_U5ojeN6ZO4I_qvCQ\",\n",
    "#  http_client=client\n",
    "# )\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# embedding_model = OpenAIEmbeddings(\n",
    "#  base_url=\"https://genailab.tcs.in\",\n",
    "#  model=\"azure/genailab-maas-text-embedding-3-large\",\n",
    "#  api_key=\"sk-A5iYc_U5ojeN6ZO4I_qvCQ\",\n",
    "#  http_client=client)\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "# import requests\n",
    "# for method in (\"get\",\"post\",\"put\",\"delete\",\"head\",\"options\",\"patch\"):\n",
    "#     original = getattr(requests,method)\n",
    "\n",
    "#     def insecure_request(*args, _original = original, **kwargs):\n",
    "#         kwargs[\"verify\"] = False\n",
    "#         return _original(*args,**kwargs)\n",
    "    \n",
    "#     setattr(requests,method,insecure_request)\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# loader = PyPDFLoader(r\"c:\\Users\\Genaiblrpiousr13\\Downloads\\test\\checking\\files\\photos.pdf\")\n",
    "# docs = loader.load()\n",
    "# full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant that chunks documents for semantic search.\"),\n",
    "#     (\"human\", \"Split the following document into semantically meaningful sections. Return each chunk as a numbered list.\\n\\n{document}\")\n",
    "# ])\n",
    "\n",
    "# formatted_prompt = prompt.format_messages(document=full_text)\n",
    "\n",
    "# response = llm.invoke(formatted_prompt)\n",
    "# chunks = response.content.split(\"\\n\\n\")  # crude split; refine if needed\n",
    "\n",
    "# chunked_docs = [Document(page_content=chunk.strip()) for chunk in chunks if chunk.strip()]\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# vector_store = Chroma.from_documents(\n",
    "#     chunked_docs,\n",
    "#     embedding_model,\n",
    "#     persist_directory=\"chroma_db\"\n",
    "# )\n",
    "# vector_store.persist()\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.runnables import RunnableMap\n",
    "\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant that answers questions based on retrieved documents.\"),\n",
    "#     (\"human\", \"Answer the question using the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n",
    "# ])\n",
    "\n",
    "# # Chain: retrieve → format prompt → invoke LLM\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "# )\n",
    "# query = \"Explain the Camera Handling skills\"\n",
    "# response = rag_chain.invoke(query)\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\",\n",
    "#      \"You are a forecasting assistant that predicts future monthly sales for dairy products based on:\\n\"\n",
    "#      \"1) Dairy historical sales CSV with columns: Month, Milk, Ghee, Curd, Paneer, Cheese, Sweets.\\n\"\n",
    "#      \"2) External market documents: Food_Beverages_Events_2025_2026.pdf, India_FMCG_outlook_2025.pdf, \"\n",
    "#      \"Market_news_on_FMCG.pdf, Retail_marketing_calendar_2026.pdf, \"\n",
    "#      \"Snipp_2025_Retail_Seasonal_Promotion_Marketing_Calendar.pdf, Dairy_Historical_Sales.csv.\\n\\n\"\n",
    "#      \"Core rules:\\n\"\n",
    "#      \"- Use retrieved context only. Do not invent data.\\n\"\n",
    "#      \"- Respect the requested products: if the question mentions one product, return only that product; if multiple, return each product separately.\\n\"\n",
    "#      \"- For each product predicted, include the last 10 months of its historical sales explicitly in the output.\\n\"\n",
    "#      \"- Incorporate seasonal patterns, event calendars, promotions, and market news as key drivers.\\n\"\n",
    "#      \"- Flag risks or uncertainty (e.g., supply chain issues, policy changes, macro slowdown) when present in the context.\\n\"\n",
    "#      \"- If the question specifies months/years, predict for exactly those. If not, predict for the next 3 future months beyond the latest historical date.\\n\"\n",
    "#      \"- Handle date parsing robustly: Month is YYYY-MM, Year is the 4-digit year.\\n\"\n",
    "#      \"- If any required context for a product/month is missing, state 'Insufficient context' for Predicted Demand Sales and explain in Key Drivers.\\n\"\n",
    "#      \"- Output must be strictly tabular with these columns and no extra commentary:\\n\"\n",
    "#      \"  Month | Year | Product | Last 10 Months Sales | Predicted Demand Sales | Key Drivers | Risks/Uncertainty.\\n\"),\n",
    "#     (\"human\",\n",
    "#      \"Using the following combined context (historical CSV + external market documents):\\n\\n{context}\\n\\n\"\n",
    "#      \"Task:\\n\"\n",
    "#      \"1) Identify the products and months mentioned in the question.\\n\"\n",
    "#      \"2) For each product, list the last 10 months of historical sales from the CSV.\\n\"\n",
    "#      \"3) Predict the sales for the requested months using historical trends and external drivers.\\n\"\n",
    "#      \"4) Provide concise Key Drivers (seasonality, events, promotions, market news) and note Risks/Uncertainty when applicable.\\n\\n\"\n",
    "#      \"Return only a table with the exact columns specified, no prose.\\n\\n\"\n",
    "#      \"Question: {question}\")\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \n",
    "#      \"You are a helpful assistant that predicts future sales for one or more products \"\n",
    "#      \"based on Dairy historical sales data and supporting market documents. \"\n",
    "#      \"You must use the retrieved context from multiple sources \"\n",
    "#      \"(Dairy historical sales CSV with products: Milk, Ghee, Curd, Paneer, Cheese, Sweets; \"\n",
    "#      \"1. Food_Beverages_Events_2025_2026.pdf 2. India_FMCG_outlook_2025.pdf 3. Market_news_on_FMCG.pdf 4. Retail_marketing_calendar_2026.pdf 5. Snipp_2025_Retail_Seasonal_Promotion_Marketing_Calendar.pdf 6. Dairy_Historical_Sales.csv) \"\n",
    "#      \"to identify trends, seasonality, promotions, and external factors that may impact sales. \"\n",
    "#      \"When a prediction is requested:\\n\"\n",
    "#      \" - If the question mentions a single product, return predictions only for that product from Dairy Historical sales CSV.\\n\"\n",
    "#      \" - If the question mentions multiple products, return predictions product‑wise from Dairy Historical sales CSV.\\n\"\n",
    "#      \" - A short explanation mentioning seasonal effects, holidays and market news from the supporting market document(1. Food_Beverages_Events_2025_2026.pdf 2. India_FMCG_outlook_2025.pdf 3. Market_news_on_FMCG.pdf 4. Retail_marketing_calendar_2026.pdf 5. Snipp_2025_Retail_Seasonal_Promotion_Marketing_Calendar.pdf 6. Dairy_Historical_Sales.csv).\\n\"\n",
    "#      \" - Highlight if any risks or uncertainty to watch for as an alert.\\n\"\n",
    "#      \" - Always include the last 10 months of historical sales and continue to provide the predicted data of given months in the query.\\n\"\n",
    "#      \" - Always return the output in a clear tabular format with columns:\\n\"\n",
    "#      \"   Month | Year | Product | Predicted Demand Sales | Key Drivers.\"),\n",
    "#     (\"human\", \n",
    "#      \"Using the following context from all available documents:\\n\\n{context}\\n\\n\"\n",
    "#      \"Predict the sales for the product(s) and months mentioned in the question. \"\n",
    "#      \"Return the results strictly in tabular format, including the last 10 months of sales \"\n",
    "#      \"and key drivers for each prediction.\\n\\n\"\n",
    "#      \"Question: {question}\")\n",
    "# ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
